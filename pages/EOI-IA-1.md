- Evolución de la inteligencia artificial aplicada a la generación de imágenes
- Herramientas de IA imagen
	- Elección de la herramienta concreta
		- Principio de pareto.
		- Directorios de aplicaciones basadas en IA
		- Herramientas propietarias versus Open Source
		- SaaS, API o local
		- Para utilizar en tu trabajo
		- Para utilizar en esta clase
	- Preparación del entorno del trabajo
- Funcionalidades típicas de la inteligencia artificial para la generación de imagen
	- Creación de imágenes a partir de pronts de texto
	- Creación de imágenes a partir de imágenes
	- Edición de imágenes con out paint e in painting.
	- Creación de personajes consistentes
	- Quitar el fondo a una imagen
	- Crear una imagen de producto a partir de una foto de mala calidad.
	- Aumentar la resolución de una imagen
	- Generar un modelo 3D a partir de una descripción de texto
	- Generación de texto dentro de una imagen
	- Creación de logos
	-
- Automatización de creación de imágenes
- Consideraciones éticas ilegales de la generación de imágenes con IA.
	- Sesgos en la generación de imágenes con IA.
- ## Tecnologías
	- Tecnologías de inteligencia artificial imagen Deep Research Gemini
		- # Inteligencia Artificial en Procesamiento, Edición y Generación de Imágenes: Fundamentos, Técnicas y Aplicaciones
		- ## Introducción
		  
		  La inteligencia artificial (IA), y en particular el aprendizaje profundo (deep learning, DL), ha catalizado una transformación radical en el campo de la visión por computador. Las capacidades para analizar, modificar y crear información visual han alcanzado niveles sin precedentes, impactando una amplia gama de dominios, desde la industria creativa y el entretenimiento hasta la medicina y la planificación urbana. Este documento ofrece una exploración exhaustiva del estado del arte de la IA aplicada al procesamiento, edición y generación de imágenes. Se abordarán los fundamentos teóricos, las arquitecturas de modelos clave, las tareas específicas, las herramientas disponibles, las consideraciones sobre datos y evaluación, las aplicaciones prácticas y las implicaciones éticas, legales y sociales. El objetivo es proporcionar una comprensión profunda y estructurada de este campo dinámico, dirigida a estudiantes avanzados y profesionales de la IA y la visión por computador.
		- ## Sección 1: Fundamentos de la IA en la Comprensión de Imágenes
		- ### 1.1 Visión Tradicional vs. Visión Basada en Aprendizaje Profundo: Diferencias Clave
		  
		  La visión por computador (Computer Vision, CV) es el campo de estudio que busca desarrollar técnicas para que las computadoras puedan "ver" y comprender el contenido de imágenes y videos digitales.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->1<!----><!----><!----><!----><!----><!----><!----><!----><!----> Históricamente, la CV se basaba en algoritmos de procesamiento de imágenes explícitamente programados y basados en reglas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> Estos métodos tradicionales implican pasos definidos como el preprocesamiento de imágenes (mediante filtros como el desenfoque gaussiano o la detección de bordes con Sobel o Canny) y la extracción manual de características relevantes (utilizando descriptores como SIFT, SURF o HOG).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->4<!----><!----><!----><!----><!----><!----><!----><!----><!----> Otras herramientas clave incluyen operaciones morfológicas, umbralización de color, cálculo de geometrías, transformaciones ópticas y enfoque en texturas o esquinas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->2<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  Estos enfoques tradicionales tienen ventajas significativas en ciertos contextos. Son efectivos para tareas más simples y bien definidas, a menudo requieren menos datos para funcionar adecuadamente y son computacionalmente menos exigentes, lo que los hace adecuados para aplicaciones en tiempo real o en dispositivos con recursos limitados (edge computing).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> Además, su naturaleza algorítmica explícita los hace más interpretables ("caja blanca"), permitiendo una comprensión clara de cómo se llega a un resultado.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> En situaciones donde el problema puede resolverse con técnicas simples, como la umbralización de color, los métodos tradicionales pueden ser más precisos y eficientes que el DL.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  Sin embargo, la CV tradicional enfrenta limitaciones importantes. La ingeniería de características es un proceso manual que requiere un conocimiento experto del dominio y un ajuste fino considerable de parámetros.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->4<!----><!----><!----><!----><!----><!----><!----><!----><!----> Estos métodos tienen dificultades para manejar la alta variabilidad de los objetos (orientación, iluminación), patrones complejos y la generalización a grandes conjuntos de datos o tareas nuevas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->4<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  La llegada del aprendizaje profundo (DL) marcó un cambio de paradigma. El DL, un subconjunto del aprendizaje automático (Machine Learning, ML), utiliza redes neuronales profundas (Deep Neural Networks, DNNs) para aprender representaciones y características directamente de los datos, a menudo de manera extremo a extremo (end-to-end).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->2<!----><!----><!----><!----><!----><!----><!----><!----><!----> En lugar de que un ingeniero diseñe manualmente los filtros o descriptores, las redes neuronales, especialmente las Redes Neuronales Convolucionales (CNNs), aprenden automáticamente las características más relevantes y descriptivas para la tarea específica.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->2<!----><!----><!----><!----><!----><!----><!----><!----><!----> Este aprendizaje puede ser supervisado (con datos etiquetados), no supervisado (sin etiquetas) o semi-supervisado (combinación de ambos).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->2<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  El DL ha demostrado un rendimiento y una precisión significativamente superiores en tareas complejas de CV como la clasificación de imágenes, la detección de objetos y la segmentación semántica, dominando el campo desde la irrupción de AlexNet en 2012.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> Los modelos de DL son más flexibles y adaptables a diversas tareas y dominios, ya que pueden ser reentrenados con conjuntos de datos personalizados.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  No obstante, el DL también tiene sus desventajas. Requiere grandes cantidades de datos de entrenamiento ("hambriento de datos") y una potencia computacional considerable, especialmente para el entrenamiento, lo que puede ser costoso.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->2<!----><!----><!----><!----><!----><!----><!----><!----><!----> Además, la naturaleza de las características aprendidas automáticamente a menudo hace que los modelos de DL sean menos interpretables ("caja negra").<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  Una observación fundamental es que la distinción entre CV tradicional y DL no siempre es binaria. Los enfoques híbridos a menudo ofrecen soluciones pragmáticas y eficientes. La CV tradicional puede utilizarse para preprocesamiento rápido o detección inicial (por ejemplo, detección de rostros), cuyos resultados alimentan a un modelo de DL para un análisis más complejo (por ejemplo, reconocimiento facial).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esta sinergia aprovecha la velocidad e interpretabilidad de los métodos tradicionales para subtareas más simples y la potencia del DL para el reconocimiento de patrones complejos. Esto sugiere una tendencia hacia sistemas de IA modulares que equilibran eficiencia y precisión según las restricciones del problema, como la computación en el borde versus la nube, o las necesidades de latencia versus exactitud.<!----><!----><!----><!----><!---->
		  
		  Otro punto clave derivado de esta transición es la evolución del rol del ingeniero de visión por computador. La experiencia en la creación manual de características (como SIFT o HOG) ha perdido relevancia, siendo reemplazada por la necesidad de diseñar, entrenar, evaluar e iterar sobre arquitecturas de aprendizaje profundo.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->6<!----><!----><!----><!----><!----><!----><!----><!----><!----> El enfoque se ha desplazado del *diseño de características* al *diseño de arquitecturas*, la gestión de grandes conjuntos de datos, el ajuste de hiperparámetros y las prácticas de MLOps. Esto implica una reorientación necesaria en la formación y el desarrollo de habilidades para los profesionales modernos de la CV.<!----><!----><!----><!----><!---->
		  
		  **Tabla 1: Comparación: Visión por Computador Tradicional vs. Basada en Aprendizaje Profundo**
		  
		  | **Característica** | **Visión por Computador Tradicional** | **Visión por Computador Basada en Aprendizaje Profundo** |
		  | **Ingeniería de Features** | Manual, basada en conocimiento experto (SIFT, HOG, filtros) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->4<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Automática, aprendida de los datos (end-to-end) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->4<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Requisitos de Datos** | Funciona bien con datos limitados <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->4<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Requiere grandes conjuntos de datos para entrenamiento <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->2<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Coste Computacional** | Generalmente menos exigente, apta para tiempo real/edge <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Alta demanda computacional, especialmente en entrenamiento <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->2<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Rendimiento (Tareas Simples)** | Efectiva, a veces preferible <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Puede ser excesiva ("overkill") <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Rendimiento (Tareas Complejas)** | Limitada <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->4<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Rendimiento superior (clasificación, detección, segmentación) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Interpretabilidad** | Más interpretable ("caja blanca") <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> | A menudo considerada "caja negra" <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Flexibilidad/Adaptabilidad** | Limitada, a menudo específica de la tarea <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Alta flexibilidad, generalizable a varias tareas <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->3<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Enfoque de Desarrollo** | Ingeniería manual de características <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->4<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Diseño de arquitectura, gestión de datos, entrenamiento <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->6<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  
		  <!----><!----><!----><!----><!---->
		- ### 1.2 Redes Neuronales Convolucionales (CNNs): Arquitectura y Aplicaciones
		  
		  Las Redes Neuronales Convolucionales (CNNs) son una clase especializada de redes neuronales profundas que han demostrado ser excepcionalmente efectivas para procesar datos con una topología similar a una cuadrícula, como las imágenes.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->7<!----><!----><!----><!----><!----><!----><!----><!----><!----> Su diseño está inspirado en la corteza visual animal, imitando el procesamiento jerárquico y el concepto de campos receptivos locales.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->9<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las CNNs revolucionaron el reconocimiento de imágenes y la visión por computador, superando a los métodos tradicionales en una amplia gama de tareas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->12<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Arquitectura Central de una CNN <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->12<!----><!----><!----><!----><!----><!----><!----><!----><!---->:**<!----><!----><!----><!----><!---->
		  
		  Una CNN típica se compone de varias capas apiladas:
		- **Capa de Entrada:** Recibe los datos brutos de la imagen, generalmente como una matriz tridimensional de píxeles (Alto x Ancho x Canales de Color, p.ej., RGB).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Capas Convolucionales:** Son el bloque de construcción fundamental.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->12<!----><!----><!----><!----><!----><!----><!----><!----><!----> Aplican un conjunto de filtros aprendibles (también llamados kernels) a la imagen de entrada o al mapa de características de la capa anterior.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----> Cada filtro es una pequeña matriz de pesos (p.ej., 3x3 o 5x5) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->12<!----><!----><!----><!----><!----><!----><!----><!----><!----> que se desliza (convoluciona) sobre la entrada, calculando productos punto entre los pesos del filtro y los parches locales de la entrada.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->12<!----><!----><!----><!----><!----><!----><!----><!----><!----> La salida de un filtro es un **mapa de características** (o mapa de activación) que resalta la presencia de patrones específicos (bordes, texturas, formas simples) detectados por ese filtro.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->10<!----><!----><!----><!----><!----><!----><!----><!----><!----> Parámetros clave como el número de filtros (determina la profundidad de la salida), el *stride* (paso del deslizamiento del filtro) y el *padding* (manejo de los bordes de la imagen) definen la operación.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->12<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Funciones de Activación:** Se aplican después de las capas convolucionales para introducir no linealidad en el modelo, lo cual es crucial para aprender patrones complejos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->12<!----><!----><!----><!----><!----><!----><!----><!----><!----> Funciones comunes incluyen ReLU (Rectified Linear Unit), que es computacionalmente eficiente pero puede sufrir el problema de las "neuronas muertas"; Leaky ReLU, que mitiga este problema; y funciones más antiguas como Tanh y Sigmoid, que son más propensas a problemas de gradientes evanescentes.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Capas de Pooling (Agrupación o Submuestreo):** Reducen las dimensiones espaciales (ancho y alto) de los mapas de características.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto disminuye la cantidad de parámetros y cálculos en la red, ayuda a controlar el sobreajuste y proporciona cierto grado de invarianza a pequeñas traslaciones.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->10<!----><!----><!----><!----><!----><!----><!----><!----><!----> Los tipos más comunes son **Max Pooling**, que selecciona el valor máximo en una ventana local (a menudo preferido), y **Average Pooling**, que calcula el promedio.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->9<!----><!----><!----><!----><!----><!----><!----><!----><!----> En arquitecturas más modernas, el pooling a veces se reemplaza por convoluciones con stride.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->14<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Capas Totalmente Conectadas (Fully-Connected, FC):** Se encuentran típicamente al final de la red.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----> En estas capas, cada neurona está conectada a todas las neuronas de la capa anterior, similar a un Perceptrón Multicapa (MLP) tradicional.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->10<!----><!----><!----><!----><!----><!----><!----><!----><!----> Su función es interpretar las características de alto nivel extraídas por las capas convolucionales y de pooling para realizar la tarea final, como la clasificación.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----> A menudo utilizan una función de activación Softmax en la capa de salida para problemas de clasificación, produciendo probabilidades para cada clase.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->12<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Conceptos Clave de las CNNs:**
		- **Campo Receptivo (Receptive Field):** El área restringida de la capa anterior a la que una neurona está conectada. Permite a la red enfocarse en patrones locales.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->10<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Compartición de Pesos (Weight Sharing):** El mismo conjunto de pesos (filtro) se aplica en múltiples ubicaciones espaciales de la entrada.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto reduce drásticamente el número de parámetros en comparación con las redes totalmente conectadas, haciendo a las CNNs más eficientes y menos propensas al sobreajuste.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->10<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Aprendizaje Jerárquico de Características:** Al apilar capas, las CNNs aprenden una jerarquía de características. Las capas iniciales detectan características simples (bordes, esquinas), mientras que las capas más profundas combinan estas para reconocer características más complejas (partes de objetos, objetos completos).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Invarianza/Equivarianza a la Traslación:** Gracias a la compartición de pesos, las CNNs pueden detectar una característica independientemente de su posición exacta en la imagen.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Entrenamiento y Regularización:** Las CNNs se entrenan mediante algoritmos de optimización basados en gradientes, como el descenso de gradiente estocástico (SGD), utilizando la retropropagación (backpropagation) para calcular los gradientes.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----> Para prevenir el sobreajuste, se emplean técnicas de regularización como Dropout (desactivar neuronas aleatoriamente durante el entrenamiento), Normalización por Lotes (Batch Normalization, estabiliza la distribución de las activaciones), Aumento de Datos (crear variaciones sintéticas de los datos de entrenamiento) y Detención Temprana (Early Stopping).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Aplicaciones:** Las CNNs son la base de muchas aplicaciones de visión por computador, incluyendo clasificación de imágenes <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->6<!----><!----><!----><!----><!----><!----><!----><!----><!---->, detección de objetos <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->6<!----><!----><!----><!----><!----><!----><!----><!----><!---->, segmentación semántica <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->6<!----><!----><!----><!----><!----><!----><!----><!----><!---->, reconocimiento facial <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->11<!----><!----><!----><!----><!----><!----><!----><!----><!---->, análisis de imágenes médicas <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->4<!----><!----><!----><!----><!----><!----><!----><!----><!---->, y también se han adaptado para datos 1D (audio, señales) y 3D (vídeo, datos volumétricos).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->9<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Ventajas y Desventajas:** Sus puntos fuertes son su excelente rendimiento en análisis de imágenes, el aprendizaje automático de características jerárquicas, la robustez a variaciones espaciales y la eficiencia en parámetros gracias a la compartición de pesos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->8<!----><!----><!----><!----><!----><!----><!----><!----><!----> Sus debilidades incluyen el coste computacional, la necesidad de grandes conjuntos de datos etiquetados y los desafíos en la interpretabilidad.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->13<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  La arquitectura de las CNNs no es estática; ha evolucionado significativamente. Modelos pioneros como LeNet establecieron los conceptos básicos. Arquitecturas posteriores como AlexNet, VGG, GoogLeNet/Inception, ResNet <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->12<!----><!----><!----><!----><!----><!----><!----><!----><!---->, DenseNet <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->18<!----><!----><!----><!----><!----><!----><!----><!----><!----> y EfficientNet <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->16<!----><!----><!----><!----><!----><!----><!----><!----><!----> introdujeron innovaciones clave como redes más profundas, conexiones residuales (skip connections) para mitigar el problema de los gradientes evanescentes, módulos de "inception" con convoluciones paralelas a diferentes escalas, conexiones densas para mejorar el flujo de características y estrategias eficientes de escalado de modelos. Comprender esta evolución es fundamental para apreciar los principios de diseño detrás de las CNNs modernas.<!----><!----><!----><!----><!---->
		  
		  Un aspecto crucial del éxito de las CNNs radica en sus fuertes **sesgos inductivos (inductive biases)**, que son suposiciones incorporadas sobre la naturaleza de los datos de imagen.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----> Estos sesgos son principalmente la **localidad** (la idea de que los píxeles cercanos están más relacionados que los lejanos, abordada por los filtros locales) y la **equivarianza a la traslación** (la idea de que un patrón es el mismo independientemente de dónde aparezca en la imagen, abordada por la compartición de pesos).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----> Estos sesgos hacen que las CNNs sean muy eficientes en términos de datos para tareas de visión en comparación con arquitecturas más generales como los Transformers, especialmente cuando los conjuntos de datos son de tamaño limitado.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->21<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- ### 1.3 Vision Transformers (ViT, Swin): Arquitectura y Comparación con CNNs
		  
		  Aunque las CNNs dominaron la visión por computador durante una década, la arquitectura Transformer, originalmente desarrollada para el procesamiento del lenguaje natural (NLP) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->23<!----><!----><!----><!----><!----><!----><!----><!----><!---->, ha emergido como una alternativa potente y, en muchos casos, superior.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----> El trabajo pionero "An Image is Worth 16x16 Words" demostró que un Transformer puro, aplicado directamente a parches de imagen, podía lograr resultados de vanguardia en clasificación de imágenes, desafiando la necesidad de convoluciones.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->24<!----><!----><!----><!----><!----><!----><!----><!----><!----> Modelos posteriores como Swin Transformer adaptaron aún más la arquitectura para tareas de visión de propósito general.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Arquitectura del Vision Transformer (ViT) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->24<!----><!----><!----><!----><!----><!----><!----><!----><!---->:**<!----><!----><!----><!----><!---->
		  
		  El ViT estándar adapta la arquitectura Transformer con modificaciones mínimas para imágenes:
		- **División en Parches (Image Patching):** La imagen de entrada (H x W x C) se divide en una secuencia de parches planos no superpuestos de tamaño fijo (P x P).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->23<!----><!----><!----><!----><!----><!----><!----><!----><!----> El número de parches, N = HW/P², determina la longitud de la secuencia de entrada para el Transformer.<!----><!----><!----><!----><!---->
		- **Proyección Lineal (Linear Embedding):** Cada parche aplanado se proyecta linealmente en un espacio de embedding de dimensión D mediante una capa lineal entrenable.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->24<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto crea los "patch embeddings".<!----><!----><!----><!----><!---->
		- **Embeddings de Posición (Position Embeddings):** Dado que el mecanismo de auto-atención del Transformer es invariante a la permutación y carece de información espacial inherente <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->25<!----><!----><!----><!----><!----><!----><!----><!----><!---->, se añaden embeddings de posición aprendibles (generalmente 1D) a los patch embeddings para codificar la ubicación espacial de cada parche.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->24<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Token de Clasificación (CLS Token):** Inspirado en BERT, se antepone un embedding aprendible () a la secuencia de parches.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->23<!----><!----><!----><!----><!----><!----><!----><!----><!----> El estado final de este token después del codificador Transformer se utiliza como representación global de la imagen para la clasificación.<!----><!----><!----><!----><!---->
		- **Codificador Transformer (Transformer Encoder):** La secuencia de embeddings (parches + posición + CLS) se procesa a través de un codificador Transformer estándar.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->23<!----><!----><!----><!----><!----><!----><!----><!----><!----> Este consiste en L capas idénticas, cada una con dos subcapas:
			- **Auto-Atención Multi-Cabeza (Multi-Head Self-Attention, MSA):** Permite que cada parche interactúe ("atienda") con todos los demás parches de la secuencia.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----> Calcula pesos de atención basados en la similitud entre representaciones de Consulta (Query), Clave (Key) y Valor (Value) derivadas de los embeddings de entrada.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->24<!----><!----><!----><!----><!----><!----><!----><!----><!----> La atención multi-cabeza ejecuta múltiples mecanismos de atención en paralelo, capturando diferentes tipos de relaciones.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->25<!----><!----><!----><!----><!----><!----><!----><!----><!----> Una característica clave es que la auto-atención es global desde la primera capa, permitiendo la agregación temprana de información global.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->30<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Perceptrón Multicapa (MLP Block):** Una red neuronal feed-forward simple, típicamente con dos capas y una no linealidad GELU.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->24<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Conexiones Residuales y Normalización de Capa (LayerNorm):** Se aplica LayerNorm antes de cada subcapa y se añaden conexiones residuales después de cada subcapa para facilitar el entrenamiento de redes profundas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->24<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
		- **Cabeza de Clasificación (Output):** Una capa MLP se aplica al estado final del token CLS para producir la predicción de clase.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->24<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Arquitectura del Swin Transformer <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!---->:**<!----><!----><!----><!----><!---->
		  
		  El Swin Transformer fue diseñado para superar algunas limitaciones del ViT, como su complejidad cuadrática y la falta de una estructura jerárquica, haciéndolo más adecuado como backbone de propósito general para diversas tareas de visión.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Estructura Jerárquica:** A diferencia del ViT que mantiene una resolución constante, Swin construye mapas de características jerárquicos, similares a las CNNs.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> Comienza con parches pequeños y utiliza capas de **fusión de parches (Patch Merging)** en etapas posteriores para agrupar parches vecinos (2x2), reducir la resolución espacial a la mitad y aumentar la dimensionalidad de las características.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto permite modelar información visual a diferentes escalas y facilita la integración con arquitecturas como FPN o U-Net para tareas densas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Auto-Atención Basada en Ventanas Desplazadas (Shifted Window Self-Attention):** Para lograr eficiencia lineal, Swin calcula la auto-atención *localmente* dentro de ventanas no superpuestas (p.ej., 7x7 parches) que dividen el mapa de características (W-MSA).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto limita drásticamente el coste computacional en comparación con la atención global del ViT. Para permitir la interacción entre ventanas, en bloques Transformer consecutivos, la configuración de las ventanas se *desplaza* cíclicamente (p.ej., por la mitad del tamaño de la ventana) (SW-MSA).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las nuevas ventanas traslapadas conectan parches de ventanas anteriores, permitiendo el flujo de información global de manera eficiente.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Complejidad Lineal:** La complejidad computacional es lineal con respecto al número de parches (tamaño de la imagen) porque la auto-atención se calcula solo dentro de ventanas de tamaño fijo (M²) y el número de ventanas crece linealmente con el tamaño de la imagen.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> Una implementación eficiente con desplazamiento cíclico y enmascaramiento mantiene la eficiencia incluso con las ventanas desplazadas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Comparación: ViT vs. Swin vs. CNNs:**
		- **Sesgo Inductivo:** Las CNNs tienen fuertes sesgos de localidad y equivarianza a la traslación.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----> ViT tiene sesgos mucho más débiles, dependiendo más de los datos y el preentrenamiento a gran escala.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->22<!----><!----><!----><!----><!----><!----><!----><!----><!----> Swin reintroduce sesgos de localidad (a través de ventanas) y jerarquía (a través de la fusión de parches), haciéndolo más parecido a una CNN en este aspecto.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Atención Global vs. Local:** ViT emplea atención global desde la primera capa.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->30<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las CNNs construyen contexto global gradualmente a través de la profundidad de las capas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->30<!----><!----><!----><!----><!----><!----><!----><!----><!----> Swin utiliza atención local dentro de ventanas, pero logra interacción global mediante el desplazamiento de ventanas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Complejidad Computacional:** La atención global de ViT es cuadrática O(N²) respecto al número de parches N.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->25<!----><!----><!----><!----><!----><!----><!----><!----><!----> La convolución de las CNNs escala generalmente mejor con la resolución.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->30<!----><!----><!----><!----><!----><!----><!----><!----><!----> Swin logra complejidad lineal O(N).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Requisitos de Datos:** Los ViTs puros suelen necesitar preentrenamiento masivo (p.ej., ImageNet-21k, JFT-300M) para superar a las CNNs, debido a sus sesgos más débiles.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las CNNs funcionan mejor con conjuntos de datos más pequeños.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->22<!----><!----><!----><!----><!----><!----><!----><!----><!----> Swin, con sus sesgos reintroducidos, puede funcionar bien con preentrenamiento en ImageNet-1K.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Rendimiento:** Los ViTs superan a las ResNets en la relación rendimiento/cómputo cuando se preentrenan a gran escala.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----> Swin supera a ViT/DeiT y ResNets en varios benchmarks (clasificación, detección, segmentación).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> Los modelos híbridos CNN-Transformer también son competitivos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Adecuación a Tareas:** ViT se diseñó originalmente para clasificación; se necesitan adaptaciones para tareas densas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->32<!----><!----><!----><!----><!----><!----><!----><!----><!----> Swin se diseñó como un backbone general, adecuado para predicción densa.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las CNNs (especialmente U-Nets) siguen siendo una opción fuerte para segmentación, sobre todo en dominios específicos como imágenes médicas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->35<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  La comparación entre CNNs y Transformers ilustra un compromiso fundamental en el diseño de arquitecturas. Las CNNs tienen un sesgo inductivo alto (fuertes suposiciones sobre la estructura de la imagen) pero una varianza potencialmente menor (menos dependencia de datos masivos). Los ViTs tienen un sesgo bajo (arquitectura más general) pero una varianza alta (requieren enormes conjuntos de datos para restringir el aprendizaje).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----> Swin representa un punto intermedio, reintroduciendo sesgos espaciales para mejorar la eficiencia y los requisitos de datos en comparación con ViT. La elección de la arquitectura implica gestionar este equilibrio sesgo-varianza en función de los requisitos de la tarea y la disponibilidad de datos.<!----><!----><!----><!----><!---->
		  
		  Además, aunque inicialmente distintas, existe una tendencia hacia la convergencia y la hibridación. Modelos como Swin incorporan ideas de CNNs (jerarquía, localidad) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!---->, mientras que algunas CNNs pueden incorporar mecanismos de atención. Los modelos híbridos combinan explícitamente bloques convolucionales y Transformer.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto sugiere que el futuro puede residir en arquitecturas que combinen de manera óptima el procesamiento local basado en convoluciones con el modelado de dependencias a largo alcance mediante atención.<!----><!----><!----><!----><!---->
		  
		  **Tabla 2: Comparación: CNN vs. ViT vs. Swin Transformer**
		  
		  | **Característica** | **CNN (Convolutional Neural Network)** | **Vision Transformer (ViT)** | **Swin Transformer** |
		  | **Mecanismo Primario** | Convolución, Pooling <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->11<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Auto-atención global <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->24<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Auto-atención en ventanas desplazadas <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Sesgo Inductivo (Localidad/Jerarquía)** | Alto (localidad, equiv. traslación) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Bajo <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->22<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Moderado (localidad en ventanas, jerarquía por fusión) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Alcance de Atención** | Local (se expande con profundidad) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->30<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Global (desde la primera capa) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->30<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Local (dentro de ventanas), global a través de desplazamiento <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Complejidad (vs Tamaño Imagen)** | Generalmente lineal o casi lineal <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->30<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Cuadrática O(N²) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->25<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Lineal O(N) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Requisito de Datos (Típico)** | Moderado a grande <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->22<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Muy grande (preentrenamiento masivo) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->20<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Grande (mejor que ViT con ImageNet-1K) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Adecuación Predicción Densa** | Muy adecuada (U-Net) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->35<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Requiere adaptaciones <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->32<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Diseñado como backbone general, adecuado <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->27<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  
		  <!----><!----><!----><!----><!---->
		- ## Sección 2: Modelos Generativos para Síntesis de Imágenes
		  
		  Los modelos generativos son una clase de modelos de IA diseñados para aprender la distribución subyacente de un conjunto de datos y generar nuevas muestras que se asemejen a los datos originales.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----> En el contexto de las imágenes, estos modelos pueden crear imágenes sintéticas realistas o estilizadas. Tres familias principales dominan el panorama: Autoencoders Variacionales (VAEs), Redes Generativas Antagónicas (GANs) y Modelos de Difusión.<!----><!----><!----><!----><!---->
		- ### 2.1 Autoencoders Variacionales (VAEs): Principios y Generación en Espacio Latente  **<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!---->**
		  
		  Los Autoencoders Variacionales (VAEs) son modelos generativos que combinan la arquitectura de un autoencoder con métodos probabilísticos para aprender una representación continua y estructurada del espacio latente de los datos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Arquitectura y Funcionamiento <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!---->:**<!----><!----><!----><!----><!---->
		  
		  Un VAE consta de dos redes neuronales principales:
		- **Encoder (Codificador):** Esta red toma un dato de entrada (p.ej., una imagen x) y, en lugar de mapearlo a un único punto en el espacio latente como un autoencoder estándar, mapea la entrada a los parámetros de una distribución de probabilidad sobre el espacio latente z.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->43<!----><!----><!----><!----><!----><!----><!----><!----><!----> Comúnmente, se asume una distribución gaussiana, por lo que el encoder produce un vector de medias (μ) y un vector de log-varianzas (log σ²) que definen la distribución q(z∣x) para esa entrada específica.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Espacio Latente y Muestreo (Reparameterization Trick):** El espacio latente es un espacio continuo de baja dimensión donde se captura la estructura esencial de los datos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----> Para generar una representación latente z para una entrada x dada, se muestrea de la distribución q(z∣x) definida por μ y σ. Para permitir que los gradientes fluyan a través de este proceso de muestreo durante el entrenamiento (lo cual es necesario para la optimización), se utiliza el **truco de la reparametrización**. En lugar de muestrear directamente de N(μ,σ2), se muestrea un ruido ϵ de una distribución estándar (p.ej., N(0,I)) y luego se calcula z=μ+σ⊙ϵ (donde σ=exp(0.5×logσ2)).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto introduce la aleatoriedad necesaria mientras mantiene la diferenciabilidad respecto a μ y σ.<!----><!----><!----><!----><!---->
		- **Decoder (Decodificador):** Esta red toma un vector latente muestreado z como entrada y trata de reconstruir el dato original x.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----> Aprende la función de mapeo p(x∣z) desde el espacio latente de vuelta al espacio de datos.<!----><!----><!----><!----><!---->
		  
		  **Función de Pérdida <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!---->:**<!----><!----><!----><!----><!---->
		  
		  El VAE se entrena optimizando una función de pérdida que consta de dos términos, derivados de la maximización del Límite Inferior de la Evidencia (Evidence Lower Bound, ELBO) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->43<!----><!----><!----><!----><!----><!----><!----><!----><!---->:<!----><!----><!----><!----><!---->
		- **Pérdida de Reconstrucción:** Mide qué tan bien el decodificador reconstruye la entrada original x a partir del vector latente z muestreado de q(z∣x).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----> Típicamente se usa el Error Cuadrático Medio (MSE) para datos continuos o la Entropía Cruzada Binaria para datos binarios.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----> Este término asegura la fidelidad de la reconstrucción.<!----><!----><!----><!----><!---->
		- **Pérdida de Divergencia KL (Kullback-Leibler):** Mide la diferencia entre la distribución latente aprendida por el encoder q(z∣x) y una distribución *a priori* p(z), que generalmente se elige como una gaussiana estándar N(0,I).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----> Este término actúa como un regularizador, forzando a las distribuciones latentes de las entradas a parecerse a la distribución *a priori*. Esto es crucial porque asegura que el espacio latente sea continuo y esté bien estructurado, permitiendo que puntos muestreados aleatoriamente de la distribución *a priori* N(0,I) generen muestras nuevas y coherentes al pasar por el decodificador.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  La pérdida total es la suma de estos dos términos: L=Lreconstruccioˊn​+LKL​.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Generación de Nuevas Muestras:**
		  
		  Una vez entrenado el VAE, se pueden generar nuevas muestras (p.ej., imágenes) muestreando un vector latente z directamente de la distribución *a priori* p(z)=N(0,I) y pasándolo a través del decodificador entrenado.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->43<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Ventajas y Desventajas:**
		- **Ventajas:** Aprenden espacios latentes suaves y continuos que son interpretables y útiles para tareas como la interpolación entre muestras, la manipulación de atributos y la detección de anomalías.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->44<!----><!----><!----><!----><!----><!----><!----><!----><!----> El proceso de entrenamiento es generalmente estable.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!----> Son útiles para el aumento de datos y la extracción de características.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->45<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Desventajas:** Las imágenes generadas por VAEs tienden a ser más borrosas o menos nítidas en comparación con las generadas por GANs, especialmente para datos de alta dimensión.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!----> Existe un compromiso inherente entre la calidad de la reconstrucción y la regularidad del espacio latente impuesta por el término KL.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!----> Pueden ser computacionalmente costosos de entrenar.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->45<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  La clave del funcionamiento generativo de los VAEs reside en la combinación del **encoder probabilístico** y la **regularización de KL divergencia**. El encoder no mapea una entrada a un punto fijo, sino a una distribución (μ, σ), introduciendo una noción de incertidumbre.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----> El término KL fuerza a estas distribuciones individuales a agruparse cerca de una distribución *a priori* simple (normalmente N(0,I)).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esta regularización evita que el encoder "memorice" las entradas asignándoles regiones disjuntas en el espacio latente; en cambio, fomenta un espacio latente denso y continuo. Como resultado, cualquier punto muestreado de la distribución *a priori* tiene una alta probabilidad de caer cerca de la distribución latente de alguna entrada real, lo que permite al decodificador generar una salida plausible y coherente, habilitando así la generación de nuevas muestras.<!----><!----><!----><!----><!---->
		- ### 2.2 Redes Generativas Antagónicas (GANs): Framework, Arquitecturas y Estabilidad
		  
		  Las Redes Generativas Antagónicas (Generative Adversarial Networks, GANs), introducidas por Ian Goodfellow en 2014 <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->50<!----><!----><!----><!----><!----><!----><!----><!----><!---->, representan un enfoque fundamentalmente diferente para el modelado generativo, basado en un juego adversarial entre dos redes neuronales.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->50<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Framework Central:**
		  
		  Una GAN consta de dos componentes que compiten entre sí <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->51<!----><!----><!----><!----><!----><!----><!----><!----><!---->:<!----><!----><!----><!----><!---->
		- **Generador (G):** Una red neuronal que toma como entrada un vector de ruido aleatorio z (muestreado de una distribución latente simple, p.ej., Gaussiana) y trata de generar datos sintéticos (p.ej., imágenes) que sean indistinguibles de los datos reales del conjunto de entrenamiento.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->50<!----><!----><!----><!----><!----><!----><!----><!----><!----> El objetivo de G es "engañar" al Discriminador.<!----><!----><!----><!----><!---->
		- **Discriminador (D):** Una red neuronal que actúa como un clasificador binario. Recibe como entrada tanto datos reales del conjunto de entrenamiento como datos falsos generados por G, y su objetivo es distinguir correctamente entre los dos, asignando una alta probabilidad a los datos reales y una baja probabilidad a los datos falsos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->50<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Entrenamiento Adversarial:**
		  
		  El entrenamiento es un proceso iterativo y competitivo, formulado como un juego minimax <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->50<!----><!----><!----><!----><!----><!----><!----><!----><!---->:<!----><!----><!----><!----><!---->
		- El **Discriminador (D)** se entrena para maximizar su capacidad de clasificación, es decir, maximizar la probabilidad de etiquetar correctamente las muestras reales como reales (logD(x)) y las muestras falsas (generadas por G, G(z)) como falsas (log(1−D(G(z)))).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->50<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- El **Generador (G)** se entrena para minimizar la probabilidad de que el Discriminador detecte sus salidas como falsas, es decir, minimizar log(1−D(G(z))).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->51<!----><!----><!----><!----><!----><!----><!----><!----><!----> En la práctica, a menudo se utiliza un objetivo alternativo no saturante, que es maximizar logD(G(z)), para mejorar la estabilidad de los gradientes al principio del entrenamiento.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->50<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- Las redes se entrenan alternativamente. Cuando se entrena D, los pesos de G se mantienen fijos, y viceversa. Los gradientes del error de clasificación de D se retropropagan para actualizar los pesos de D. Crucialmente, los gradientes de la salida de D con respecto a las muestras generadas también se utilizan para actualizar los pesos de G, proporcionando la señal de aprendizaje para que G mejore sus generaciones.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->51<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- El proceso idealmente converge a un **equilibrio de Nash**, donde el Generador produce muestras tan realistas que el Discriminador no puede distinguirlas de las reales (su precisión es del 50%, como lanzar una moneda).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->50<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Arquitecturas GAN Clave:**
		- **DCGAN (Deep Convolutional GAN):** Fue una de las primeras arquitecturas en aplicar CNNs con éxito a las GANs, estableciendo directrices arquitectónicas para estabilizar el entrenamiento.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->55<!----><!----><!----><!----><!----><!----><!----><!----><!----> Estas incluyen <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->14<!----><!----><!----><!----><!----><!----><!----><!----><!---->:
			- Reemplazar capas de pooling por convoluciones con stride (en D) y convoluciones transpuesta/fraccionalmente strided (en G).
			- Usar Normalización por Lotes (Batch Normalization, BN) en ambas redes (excepto en la capa de salida de G y la capa de entrada de D).
			- Eliminar capas ocultas totalmente conectadas en arquitecturas profundas.
			- Usar activación ReLU en G (excepto la salida, que usa Tanh) y LeakyReLU en D.
			- El generador típicamente toma un vector latente de 100 dimensiones. DCGAN ayudó a mitigar problemas como el colapso de modos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->55<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
		- **StyleGAN / StyleGAN2:** Representan el estado del arte en síntesis de imágenes de alta resolución y fidelidad, especialmente para rostros humanos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->57<!----><!----><!----><!----><!----><!----><!----><!----><!----> Sus innovaciones clave se centran en mejorar el control sobre el proceso de generación y la calidad de la imagen <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->57<!----><!----><!----><!----><!----><!----><!----><!----><!---->:
			- **Red de Mapeo (Mapping Network):** Transforma el vector latente inicial z en un espacio latente intermedio W (usando una MLP). Se argumenta que W está menos "entrelazado" que Z, permitiendo un control más semántico sobre los atributos de la imagen generada.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->59<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Modulación de Estilo (AdaIN / Weight Modulation):** El vector de estilo w∈W controla la síntesis en diferentes niveles de resolución de la red generadora. En StyleGAN1, esto se hacía mediante la Normalización Adaptativa de Instancia (AdaIN), que ajusta la media y la varianza de los mapas de características basándose en w.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->59<!----><!----><!----><!----><!----><!----><!----><!----><!----> StyleGAN2 reemplazó AdaIN (que causaba artefactos tipo gota <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->57<!----><!----><!----><!----><!----><!----><!----><!----><!---->) por una modulación directa de los pesos convolucionales basada en w, seguida de una demodulación (normalización) para preservar la escala de las características.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->57<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto permite un control jerárquico: los estilos inyectados en capas tempranas afectan características globales (pose, forma), mientras que los inyectados en capas tardías afectan detalles finos (color, textura).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->65<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Inyección de Ruido (Noise Injection):** Se añade ruido gaussiano estocástico a diferentes capas del generador.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->57<!----><!----><!----><!----><!----><!----><!----><!----><!----> Este ruido introduce variaciones finas y aleatorias (como la posición del pelo, pecas) sin afectar la estructura global o la identidad, aumentando el realismo.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->64<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Crecimiento Progresivo (Progressive Growing, StyleGAN1):** El entrenamiento comenzaba con baja resolución y se añadían gradualmente capas para aumentar la resolución.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->59<!----><!----><!----><!----><!----><!----><!----><!----><!----> Se eliminó en StyleGAN2 porque introducía artefactos (como la localización preferente de características).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->57<!----><!----><!----><!----><!----><!----><!----><!----><!----> Fue reemplazado por una arquitectura con conexiones skip (inspirada en MSG-GAN) o residuales.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->60<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Regularización de Longitud de Camino (Path Length Regularization, PPL, StyleGAN2):** Un término de regularización añadido a la pérdida del generador.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->60<!----><!----><!----><!----><!----><!----><!----><!----><!----> Fomenta que un paso de tamaño fijo en el espacio latente W resulte en un cambio de magnitud fija en la imagen generada.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->66<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto mejora la suavidad y el desentrelazamiento del espacio latente, facilitando interpolaciones más suaves y la inversión de imágenes reales al espacio latente.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->57<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Inversión GAN (GAN Inversion):** La arquitectura de StyleGAN, especialmente StyleGAN2 con PPL, es propicia para la inversión, es decir, encontrar el vector latente w que reconstruye una imagen real dada. Esto permite editar imágenes reales manipulando su código latente.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->57<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
		- **BigGAN:** Se centró en escalar las GANs para generar imágenes de alta fidelidad y diversidad en conjuntos de datos complejos como ImageNet a altas resoluciones.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->69<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las técnicas clave incluyen <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->71<!----><!----><!----><!----><!----><!----><!----><!----><!---->:
			- **Escalado Masivo:** Aumentaron significativamente el número de parámetros del modelo (2-4x) y el tamaño del lote (batch size) (8x) en comparación con trabajos anteriores.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->69<!----><!----><!----><!----><!----><!----><!----><!----><!----> Lotes más grandes proporcionan mejores gradientes para G y D.<!----><!----><!----><!----><!---->
			- **Regularización Ortogonal:** Aplicada a los pesos del generador para mejorar la estabilidad del entrenamiento a gran escala y el condicionamiento, lo que permitió el uso efectivo del "truco de truncamiento".<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->69<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Truco de Truncamiento (Truncation Trick):** Durante la inferencia, en lugar de muestrear z de una gaussiana estándar, se muestrea de una gaussiana truncada (valores fuera de un rango [-t, t] se re-muestrean).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->69<!----><!----><!----><!----><!----><!----><!----><!----><!----> Reducir el umbral de truncamiento (acercar z a 0) mejora la fidelidad (calidad) de las muestras individuales a costa de reducir la variedad (diversidad) general de las muestras generadas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->69<!----><!----><!----><!----><!----><!----><!----><!----><!----> Permite un control explícito sobre este compromiso calidad-diversidad.<!----><!----><!----><!----><!---->
			- **Otras Mejoras:** Incorporaron módulos de auto-atención (SAGAN) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->69<!----><!----><!----><!----><!----><!----><!----><!----><!---->, embeddings compartidos para el condicionamiento de clase y conexiones skip desde z a múltiples capas (skip-z).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->69<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
			  
			  **Problemas de Estabilidad en el Entrenamiento y Soluciones:**
			  
			  El entrenamiento de GANs es inherentemente inestable <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!---->, presentando desafíos como:<!----><!----><!----><!----><!---->
		- **Gradientes Evanescentes (Vanishing Gradients):** Si D se vuelve demasiado bueno rápidamente, los gradientes para G pueden volverse cero o muy pequeños, deteniendo el aprendizaje de G.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->50<!----><!----><!----><!----><!----><!----><!----><!----><!----> A menudo relacionado con la saturación de la divergencia JS en la formulación original de GAN.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->54<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Colapso de Modos (Mode Collapse):** G aprende a generar solo un subconjunto limitado de los tipos de datos presentes en el conjunto de entrenamiento, fallando en capturar la diversidad completa de la distribución real.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **No Convergencia / Oscilaciones:** G y D pueden oscilar sin alcanzar un punto de equilibrio estable.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->78<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  Se han desarrollado numerosas técnicas para mitigar estos problemas:
		- **Directrices Arquitectónicas (DCGAN):** Uso de convoluciones strided/transpuestas, BN, activaciones específicas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->15<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Modificaciones de la Función de Pérdida:**
			- *Pérdida No Saturante para G:* Maximizar logD(G(z)).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->50<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *Least Squares GAN (LSGAN):* Usar pérdida L2 en lugar de entropía cruzada binaria.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->79<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *Wasserstein GAN (WGAN):* Utiliza la distancia de Wasserstein (Earth Mover's Distance) como función de pérdida.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->50<!----><!----><!----><!----><!----><!----><!----><!----><!----> Tiene mejores propiedades teóricas (gradientes más estables, mejor correlación con la calidad de imagen) y es menos propenso al colapso de modos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->81<!----><!----><!----><!----><!----><!----><!----><!----><!----> Requiere que el discriminador (llamado "crítico" en WGAN) satisfaga la restricción de Lipschitz 1.<!----><!----><!----><!----><!---->
		- **Aplicación de la Restricción de Lipschitz (para WGAN):**
			- *Recorte de Pesos (Weight Clipping, WGAN original):* Limita los pesos del crítico a un rango pequeño [-c, c].<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->54<!----><!----><!----><!----><!----><!----><!----><!----><!----> Es simple pero puede llevar a un uso ineficiente de la capacidad del modelo (pesos concentrados en los extremos) y a gradientes explosivos/evanescentes si c no se ajusta cuidadosamente.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->75<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *Penalización de Gradiente (Gradient Penalty, WGAN-GP):* Añade un término a la pérdida del crítico que penaliza la desviación de la norma del gradiente del crítico (con respecto a su entrada) de 1.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->54<!----><!----><!----><!----><!----><!----><!----><!----><!----> Se calcula sobre muestras interpoladas entre datos reales y falsos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->54<!----><!----><!----><!----><!----><!----><!----><!----><!----> Es más estable y efectivo que el recorte de pesos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->54<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Técnicas de Normalización:**
			- *Batch Normalization (BN):* Ampliamente utilizada (p.ej., DCGAN), pero puede introducir artefactos o dependencias no deseadas entre las muestras de un lote. Generalmente no se usa en el crítico de WGAN-GP.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->75<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *Spectral Normalization (SN):* Normaliza las matrices de pesos de una capa dividiéndolas por su norma espectral (el mayor valor singular).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->73<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto controla la constante de Lipschitz del discriminador, estabilizando el entrenamiento.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->73<!----><!----><!----><!----><!----><!----><!----><!----><!----> Es computacionalmente ligera <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->87<!----><!----><!----><!----><!----><!----><!----><!----><!----> y ayuda a prevenir gradientes explosivos/evanescentes.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->73<!----><!----><!----><!----><!----><!----><!----><!----><!----> Utilizada en SN-GAN, SAGAN, BigGAN y el discriminador de StyleGAN2.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->69<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Otras Técnicas:** Añadir ruido a las entradas del discriminador (Instance Noise) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->79<!----><!----><!----><!----><!----><!----><!----><!----><!---->, penalizar la norma del gradiente cerca de los datos reales (DRAGAN) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->79<!----><!----><!----><!----><!----><!----><!----><!----><!---->, considerar múltiples pasos futuros del oponente (Unrolled GANs) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->79<!----><!----><!----><!----><!----><!----><!----><!----><!---->, usar una formulación relativa en el discriminador (Relativistic GAN, usado en ESRGAN).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  Un aspecto importante que emerge de arquitecturas como StyleGAN es la tensión entre **control y estabilidad/calidad**. La introducción de mecanismos sofisticados para un control detallado (red de mapeo, modulación de estilo, inyección de ruido) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->59<!----><!----><!----><!----><!----><!----><!----><!----><!----> puede, a su vez, generar nuevos artefactos (como las "gotas" en StyleGAN1 <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->57<!----><!----><!----><!----><!----><!----><!----><!----><!---->) o requerir nuevas técnicas de regularización (como PPL en StyleGAN2 <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->60<!----><!----><!----><!----><!----><!----><!----><!----><!---->) para mantener la calidad de la generación y la estabilidad del espacio latente. La evolución de StyleGAN1 a StyleGAN2 ilustra este ciclo iterativo de añadir control y luego abordar los problemas resultantes para recuperar la estabilidad y la calidad.<!----><!----><!----><!----><!---->
		  
		  Otro punto relevante es la **hipótesis del escalado**. BigGAN demostró empíricamente que escalar las GANs (más parámetros, lotes más grandes) mejora drásticamente el rendimiento en conjuntos de datos complejos como ImageNet.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->69<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto respalda la idea de que "más grande es mejor", común en el aprendizaje profundo. Sin embargo, este escalado también reveló nuevas inestabilidades de entrenamiento específicas de la gran escala, que necesitaron el desarrollo de técnicas como la regularización ortogonal y el truco de truncamiento para ser manejadas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->69<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto indica que el escalado efectivo requiere no solo más recursos computacionales, sino también el codesarrollo de técnicas de regularización y entrenamiento adaptadas a esa escala.<!----><!----><!----><!----><!---->
		- ### 2.3 Modelos de Difusión: DDPM, Stable Diffusion y Superioridad Arquitectónica
		  
		  Los modelos de difusión han surgido como una clase poderosa y cada vez más dominante de modelos generativos, a menudo superando a las GANs en términos de calidad y diversidad de las muestras, aunque tradicionalmente a costa de una velocidad de muestreo más lenta.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----> Están inspirados en consideraciones de la termodinámica fuera del equilibrio.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->90<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Concepto Central y Proceso DDPM <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->91<!----><!----><!----><!----><!----><!----><!----><!----><!---->:**<!----><!----><!----><!----><!---->
		  
		  Los Modelos Probabilísticos de Difusión con Denoising (Denoising Diffusion Probabilistic Models, DDPM) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->90<!----><!----><!----><!----><!----><!----><!----><!----><!----> funcionan aprendiendo a revertir un proceso de difusión (ruido) gradual:<!----><!----><!----><!----><!---->
		- **Proceso Hacia Adelante (Fijo):** Se define un proceso de Markov fijo que gradualmente añade ruido gaussiano a una muestra de datos x0​ a lo largo de T pasos de tiempo discretos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----> En cada paso t, se añade una pequeña cantidad de ruido: q(xt​∣xt−1​)=N(xt​;αt​​xt−1​,βt​I). Los parámetros αt​ y βt​ (a menudo βt​=1−αt​) definen un "programa de ruido" (noise schedule). A medida que t aumenta, la señal original se destruye progresivamente, y para un T suficientemente grande, xT​ se aproxima a ruido gaussiano puro N(0,I).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->91<!----><!----><!----><!----><!----><!----><!----><!----><!----> Una propiedad útil es que xt​ puede muestrearse directamente a partir de x0​ usando una fórmula cerrada: q(xt​∣x0​)=N(xt​;αˉt​​x0​,(1−αˉt​)I), donde αˉt​=∏i=1t​αi​.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->91<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Proceso Inverso (Aprendido):** El objetivo es aprender la distribución inversa pθ​(xt−1​∣xt​) para poder generar datos muestreando xT​∼N(0,I) y luego muestreando iterativamente xt−1​∼pθ​(xt−1​∣xt​) para t=T,T−1,...,1.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----> Este proceso inverso también se modela como una cadena de Markov gaussiana.<!----><!----><!----><!----><!---->
		- **Arquitectura de Red:** La transición inversa pθ​(xt−1​∣xt​)=N(xt−1​;μθ​(xt​,t),Σθ​(xt​,t)) es parametrizada por una red neuronal. Comúnmente, se utiliza una arquitectura **U-Net**.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->91<!----><!----><!----><!----><!----><!----><!----><!----><!----> La U-Net toma la imagen ruidosa xt​ y el paso de tiempo t (generalmente codificado) como entrada. En lugar de predecir directamente la media μθ​, a menudo es más efectivo entrenar la red para predecir el ruido ϵ que se añadió para obtener xt​ a partir de x0​ (parametrización ϵ).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->91<!----><!----><!----><!----><!----><!----><!----><!----><!----> La varianza Σθ​ a menudo se fija a un valor predefinido (relacionado con βt​) en lugar de aprenderse.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->91<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Objetivo de Entrenamiento:** La red se entrena para predecir el ruido ϵt​ añadido en el paso t. El objetivo de pérdida simplificado, derivado del ELBO, minimiza la diferencia L2 entre el ruido real añadido y el ruido predicho por la U-Net: Lsimple​=Et,x0​,ϵ​[∣∣ϵ−ϵθ​(αˉt​​x0​+1−αˉt​​ϵ,t)∣∣2].<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->90<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Modelos de Difusión Clave y Mejoras:**
		- **DDPM:** El trabajo seminal <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->90<!----><!----><!----><!----><!----><!----><!----><!----><!----> demostró la capacidad de generar imágenes de alta calidad, superando a las GANs en algunas métricas. Sin embargo, requería miles de pasos de muestreo.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->93<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **DDIM (Denoising Diffusion Implicit Models):** Propuso un proceso de difusión no markoviano que permite un muestreo mucho más rápido (10-50x) al saltarse pasos en la trayectoria inversa, con una degradación mínima de la calidad.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->93<!----><!----><!----><!----><!----><!----><!----><!----><!----> También permite interpolaciones semánticamente significativas en el espacio latente.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->93<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Improved DDPM:** Introdujo mejoras como aprender las varianzas del proceso inverso y optimizar el programa de ruido, logrando mejores log-verosimilitudes (una métrica en la que las GANs fallan) y permitiendo un muestreo más rápido con calidad comparable.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->92<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Stable Diffusion (Latent Diffusion Model - LDM):** Una innovación arquitectónica clave que hace que los modelos de difusión sean mucho más eficientes.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->94<!----><!----><!----><!----><!----><!----><!----><!----><!----> En lugar de operar en el espacio de píxeles de alta dimensión, LDM aplica el proceso de difusión en un **espacio latente** de menor dimensión aprendido por un autoencoder (generalmente un VAE).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->94<!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *Arquitectura:* Un **VAE** (encoder E, decoder D) comprime la imagen x en una representación latente z=E(x) y la reconstruye x^=D(z). El proceso de difusión (hacia adelante y hacia atrás) se aplica a z utilizando una **U-Net** que opera en el espacio latente.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->94<!----><!----><!----><!----><!----><!----><!----><!----><!----> Para la generación condicional (p.ej., texto a imagen), se utiliza un **codificador de texto** (como CLIP ViT-L/14) para obtener un embedding del texto c, y la U-Net se condiciona en c mediante mecanismos de **atención cruzada (cross-attention)**.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->96<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *Ventajas:* Reducción drástica del coste computacional y de memoria, ya que la U-Net opera en un espacio de menor dimensión.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->98<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto permitió entrenar modelos muy potentes con recursos más accesibles y habilitó la generación de imágenes de alta resolución (p.ej., 512x512, 1024x1024) en hardware de consumo.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->96<!----><!----><!----><!----><!----><!----><!----><!----><!----> Stable Diffusion es de código abierto.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->96<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *Capacidades:* Generación de texto a imagen, edición de imagen a imagen, inpainting, outpainting.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->96<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
		- **Imagen (Google):** Modelo de texto a imagen que utiliza una **cascada de modelos de difusión en el espacio de píxeles**.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->99<!----><!----><!----><!----><!----><!----><!----><!----><!----> Un modelo base genera una imagen de baja resolución (p.ej., 64x64) condicionada por el texto, y luego uno o más modelos de difusión de súper-resolución la escalan a resoluciones más altas (p.ej., 256x256, 1024x1024), también condicionados por el texto.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->99<!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *Codificador de Texto:* Utiliza **codificadores de lenguaje grandes preentrenados y congelados** (como T5-XXL).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->99<!----><!----><!----><!----><!----><!----><!----><!----><!----> Descubrieron que escalar el tamaño del modelo de lenguaje era más beneficioso para la calidad y la alineación texto-imagen que escalar el tamaño de las U-Nets de difusión.<!----><!----><!----><!----><!---->
			- *Calidad:* Logró un fotorrealismo y una comprensión del lenguaje sin precedentes en su momento.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->99<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
		- **Stable Diffusion 3 (SD3):** Incorpora una arquitectura **Diffusion Transformer (DiT)**, reemplazando la U-Net convolucional con un Transformer para el paso de denoising.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->105<!----><!----><!----><!----><!----><!----><!----><!----><!----> Utiliza pesos separados para las representaciones de imagen y lenguaje (Multimodal Diffusion Transformer, MMDiT) para mejorar la comprensión del texto y la ortografía.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->105<!----><!----><!----><!----><!----><!----><!----><!----><!----> También combina técnicas de **flow matching**.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->105<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Superioridad Arquitectónica (vs. GANs):**
		  
		  Los modelos de difusión, especialmente desde LDM/Stable Diffusion, han demostrado ventajas significativas sobre las GANs en varios aspectos clave:
		- **Estabilidad del Entrenamiento:** Son inherentemente más estables de entrenar, ya que no implican la optimización adversarial minimax que plaga a las GANs. Evitan problemas como el colapso de modos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Calidad y Diversidad de las Muestras:** A menudo logran una mayor calidad de imagen (fidelidad, realismo) y una mejor diversidad (capturan mejor la distribución de los datos) que las GANs.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----> Métricas como FID suelen ser mejores para los modelos de difusión.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->90<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Velocidad de Muestreo:** Este fue históricamente el punto débil de los modelos de difusión, requiriendo cientos o miles de pasos de denoising.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----> Sin embargo, técnicas como DDIM <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->93<!----><!----><!----><!----><!----><!----><!----><!----><!---->, el entrenamiento en espacio latente (LDM) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->98<!----><!----><!----><!----><!----><!----><!----><!----><!----> y la destilación de modelos (p.ej., Latent Consistency Models <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->107<!----><!----><!----><!----><!----><!----><!----><!----><!---->) han reducido drásticamente el número de pasos necesarios (a veces a menos de 10), haciendo la inferencia mucho más rápida, aunque a menudo todavía no tan instantánea como una GAN.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->106<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  La innovación clave de **Stable Diffusion (LDM)** fue trasladar el costoso proceso iterativo de difusión del espacio de píxeles de alta dimensión a un espacio latente comprimido de baja dimensión, aprendido por un VAE.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->94<!----><!----><!----><!----><!----><!----><!----><!----><!----> La U-Net opera sobre estas representaciones latentes, que son mucho más pequeñas, reduciendo enormemente los requisitos computacionales y de memoria.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->98<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto hizo que la generación de imágenes de alta resolución mediante difusión fuera práctica y accesible en hardware de consumo, democratizando enormemente la tecnología.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->96<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  Por otro lado, el éxito de **Imagen** subrayó la importancia crítica de la **comprensión profunda del lenguaje** para la generación de texto a imagen de alta calidad.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->99<!----><!----><!----><!----><!----><!----><!----><!----><!----> Al utilizar modelos de lenguaje preentrenados masivos (y congelados) como T5, demostraron que mejorar el codificador de texto tenía un impacto mayor en la fidelidad y la alineación texto-imagen que mejorar la propia U-Net de difusión.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->99<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto sugiere que, para tareas condicionales complejas, la calidad de la representación de la condición (el texto) es tan importante, si not más, que la propia arquitectura generativa.<!----><!----><!----><!----><!---->
		- ### 2.4 Análisis Comparativo: VAEs vs. GANs vs. Modelos de Difusión
		  
		  Al comparar estas tres familias principales de modelos generativos de imágenes, surgen diferencias clave en sus mecanismos, fortalezas y debilidades.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Tabla 3: Comparación: VAE vs. GAN vs. Modelos de Difusión para Generación de Imágenes**
		  
		  | **Característica** | **Autoencoder Variacional (VAE)** | **Red Generativa Antagónica (GAN)** | **Modelo de Difusión** |
		  | **Mecanismo Central** | Encoder (μ, σ) + Decoder, Pérdida Reconstrucción + KL <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->41<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Generador vs. Discriminador, Entrenamiento Adversarial <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->51<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Proceso de Ruido Adelante (Fijo) + Proceso de Denoising Inverso (Aprendido, U-Net/Transformer) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->91<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Calidad de Muestra (Fidelidad)** | A menudo borrosa/menos nítida <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Alta fidelidad, fotorrealista (especialmente StyleGAN, BigGAN) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Estado del arte, a menudo superior a GANs en detalle y realismo <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Diversidad de Muestra** | Buena, muestreo de espacio latente continuo <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Propensa a colapso de modos (variedad limitada) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Excelente diversidad, menos propensa a colapso de modos <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Estabilidad del Entrenamiento** | Generalmente estable y fácil de entrenar <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Notoriamente inestable, difícil de ajustar, requiere técnicas de estabilización <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Generalmente más estable que las GANs <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->49<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Velocidad de Muestreo (Generación)** | Rápida (un paso adelante por el decoder) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->106<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Muy rápida (un paso adelante por el generador) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Tradicionalmente lenta (muchos pasos iterativos), pero mejorando rápidamente (DDIM, LDM, destilación) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Interpretabilidad Espacio Latente** | Buena, espacio suave y estructurado para interpolación <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->44<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Puede ser complejo/entrelazado (mejorado en StyleGAN) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->60<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Menos directo que VAEs, distribuido en el tiempo <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->93<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  | **Casos de Uso Típicos** | Detección anomalías, aumento datos, repr. latente <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->45<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Síntesis fotorrealista, super-resolución, deepfakes, estilo <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!----> | Texto-a-imagen SOTA, edición, inpainting, generación vídeo <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->48<!----><!----><!----><!----><!----><!----><!----><!----><!----> |
		  
		  <!----><!----><!----><!----><!---->
		  
		  Esta comparación revela lo que podría llamarse un **"Trilema Generativo"** histórico: un compromiso entre la calidad de la muestra, la diversidad/estabilidad y la velocidad de muestreo. Las GANs ofrecían alta calidad y velocidad pero sufrían de inestabilidad y colapso de modos. Los VAEs eran estables y diversos pero generaban imágenes borrosas. Los modelos de difusión inicialmente ofrecían calidad y diversidad superiores con entrenamiento estable, pero a costa de una velocidad de muestreo muy lenta. Gran parte de la investigación reciente se ha centrado en superar este trilema: estabilizar las GANs (WGAN-GP, SN), mejorar la calidad de los VAEs (p.ej., VQ-VAE <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->110<!----><!----><!----><!----><!----><!----><!----><!----><!---->) y, sobre todo, acelerar drásticamente el muestreo de los modelos de difusión (DDIM, LDM/Stable Diffusion, destilación de consistencia latente <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->93<!----><!----><!----><!----><!----><!----><!----><!----><!---->).<!----><!----><!----><!----><!---->
		  
		  Además, la práctica moderna muestra una creciente **simbiosis entre modelos**. Stable Diffusion es un ejemplo paradigmático, combinando un VAE para la compresión/descompresión del espacio latente con una U-Net de difusión que opera en ese espacio.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->94<!----><!----><!----><!----><!----><!----><!----><!----><!----> Los VAEs también se pueden combinar con GANs para mejorar la calidad (p.ej., VQ-VAE-GAN).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->47<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto sugiere que las arquitecturas generativas futuras podrían ser cada vez más híbridas, aprovechando los puntos fuertes de cada familia de modelos (p.ej., VAE para compresión eficiente, Difusión para generación robusta, quizás un discriminador GAN como pérdida perceptual) para lograr un rendimiento óptimo general.<!----><!----><!----><!----><!---->
		- ## Sección 3: Tareas de Procesamiento y Edición de Imágenes Impulsadas por IA
		  
		  Más allá de la generación de imágenes desde cero, la IA, y en particular el aprendizaje profundo, ha revolucionado una amplia gama de tareas de procesamiento y edición de imágenes.
		- ### 3.1 Super-Resolución (SR): Técnicas (ESRGAN, Real-ESRGAN) y Aplicaciones
		  
		  La Super-Resolución (SR) consiste en aumentar la resolución espacial de una imagen de baja resolución (LR) para obtener una versión de alta resolución (HR) con detalles mejorados.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!----> Es un problema inverso inherentemente mal planteado, ya que se debe inferir información de alta frecuencia que se perdió en el proceso de degradación.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  Mientras que los métodos tradicionales como la interpolación (bilineal, bicúbica) son rápidos pero tienden a producir resultados borrosos <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->112<!----><!----><!----><!----><!----><!----><!----><!----><!---->, y los métodos basados en reconstrucción son computacionalmente caros <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->114<!----><!----><!----><!----><!----><!----><!----><!----><!---->, las técnicas basadas en aprendizaje profundo, especialmente las CNNs y GANs, han logrado avances significativos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->113<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **SRGAN (Super-Resolution GAN):** Fue un trabajo pionero que utilizó una GAN para generar texturas realistas en SR.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!----> En lugar de optimizar únicamente métricas basadas en píxeles como el Error Cuadrático Medio (MSE) o la Relación Señal-Ruido Pico (PSNR), que tienden a favorecer soluciones excesivamente suaves <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->113<!----><!----><!----><!----><!----><!----><!----><!----><!---->, SRGAN introdujo una **pérdida perceptual** (calculada como la diferencia entre mapas de características de alto nivel de una red VGG preentrenada) y una **pérdida adversarial** (del discriminador GAN).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto alentó la generación de detalles finos y texturas convincentes, aunque a veces a costa de artefactos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->115<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **ESRGAN (Enhanced SRGAN):** Mejoró SRGAN para obtener una calidad visual superior.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las mejoras clave incluyeron <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!---->:
			- Una arquitectura de generador más potente con **Bloques Densos Residuales-en-Residuales (Residual-in-Residual Dense Blocks, RRDB)** y eliminación de la Normalización por Lotes (que podía introducir artefactos). Los RRDB fomentan la reutilización de características para aprender detalles finos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- Un discriminador basado en **GAN Relativista (RGAN)**, que juzga si una imagen HR real es "más realista" que una imagen SR falsa, y viceversa, estabilizando el entrenamiento adversarial.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- Una pérdida perceptual mejorada utilizando características VGG **antes** de la activación y usando distancia L1 en lugar de L2.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
		- **Real-ESRGAN:** Se enfoca en la restauración práctica de imágenes del mundo real con degradaciones complejas y desconocidas, utilizando únicamente datos sintéticos para el entrenamiento.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->117<!----><!----><!----><!----><!----><!----><!----><!----><!----> Sus contribuciones principales son <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->117<!----><!----><!----><!----><!----><!----><!----><!----><!---->:
			- **Modelo de Degradación de Alto Orden:** Para simular mejor las degradaciones del mundo real (que a menudo son una combinación de múltiples procesos como captura, compresión, transmisión), Real-ESRGAN aplica repetidamente un proceso de degradación clásico (blur, resize, noise, JPEG) con parámetros variables (p.ej., un modelo de segundo orden aplica la degradación dos veces).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->117<!----><!----><!----><!----><!----><!----><!----><!----><!----> También modela artefactos comunes como ringing y overshoot.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->117<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Discriminador U-Net con Normalización Espectral (SN):** Reemplaza el discriminador estilo VGG por una arquitectura U-Net, que puede proporcionar retroalimentación más detallada a nivel de píxel al generador.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->117<!----><!----><!----><!----><!----><!----><!----><!----><!----> La Normalización Espectral se aplica para estabilizar el entrenamiento con las degradaciones más complejas y el discriminador U-Net, ayudando a prevenir artefactos de sobre-afilado.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->117<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
			  
			  Un cambio fundamental impulsado por SRGAN y ESRGAN fue el paso de optimizar métricas de fidelidad a nivel de píxel (MSE, PSNR) a optimizar **métricas perceptuales** (pérdida perceptual, pérdida adversarial).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!----> Aunque minimizar el MSE produce imágenes que son numéricamente más cercanas al original, a menudo resultan perceptualmente insatisfactorias (borrosas) porque promedian las posibles texturas de alta frecuencia.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->113<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las pérdidas perceptuales y adversariales, al penalizar las diferencias en el espacio de características profundas o la "irrealidad" juzgada por un discriminador, empujan al generador a producir texturas más nítidas y creíbles, que son preferidas por los observadores humanos, incluso si los valores de los píxeles individuales no coinciden exactamente con la verdad terreno.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->89<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  Asimismo, Real-ESRGAN aborda explícitamente el problema de la **brecha de realidad (reality gap)**.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->117<!----><!----><!----><!----><!----><!----><!----><!----><!----> Los modelos entrenados exclusivamente con degradaciones sintéticas simples a menudo fallan estrepitosamente al aplicarse a imágenes reales, cuyas degradaciones son mucho más complejas y desconocidas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->117<!----><!----><!----><!----><!----><!----><!----><!----><!----> Al intentar modelar estas degradaciones complejas de manera más realista (a través del modelo de alto orden), Real-ESRGAN mejora significativamente la aplicabilidad práctica de la SR ciega (blind SR).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->117<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  **Aplicaciones:** La SR tiene amplias aplicaciones:
		- **Fotografía:** Mejorar fotos antiguas, aumentar la resolución para impresión, mejorar imágenes con poca luz.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->112<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Medicina:** Aumentar la resolución de imágenes médicas como resonancias magnéticas (MRI) y tomografías computarizadas (CT) para mejorar la visualización de detalles finos, ayudar en el diagnóstico y potencialmente reducir el tiempo de escaneo o la dosis de radiación.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->112<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Imagen Satelital y Teledetección:** Extraer detalles geográficos más finos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->113<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Seguridad y Vigilancia:** Mejorar la calidad de metraje de baja resolución para facilitar la identificación.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->113<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Medios y Entretenimiento:** Escalar contenido de vídeo antiguo a resoluciones modernas, reducir el ancho de banda de transmisión al escalar en el lado del cliente.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->113<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Videojuegos:** Mejorar la calidad de las texturas y los gráficos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->112<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- ### 3.2 Completado de Imágenes: Inpainting y Outpainting (LaMa, DeepFill)
		  
		  El completado de imágenes se refiere a la tarea de rellenar regiones faltantes o enmascaradas dentro de una imagen (**inpainting**) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->88<!----><!----><!----><!----><!----><!----><!----><!----><!----> o extender el lienzo de la imagen y rellenar el área recién creada (**outpainting**).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->131<!----><!----><!----><!----><!----><!----><!----><!----><!----> El objetivo es generar contenido que sea visualmente plausible y semánticamente coherente con el contexto circundante.<!----><!----><!----><!----><!---->
		  
		  Esta tarea es desafiante, especialmente cuando las regiones faltantes son grandes o tienen formas irregulares (free-form masks), ya que requiere una comprensión tanto de la estructura global de la imagen como de los detalles locales de textura.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->88<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Enfoques Tradicionales:** Incluyen métodos basados en difusión (que propagan información desde los bordes del hueco) y métodos basados en parches (que buscan y copian parches similares de las regiones conocidas).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->126<!----><!----><!----><!----><!----><!----><!----><!----><!----> Estos métodos funcionan bien para agujeros pequeños o texturas repetitivas, pero a menudo fallan en la inferencia semántica necesaria para agujeros grandes.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->129<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Enfoques Basados en Aprendizaje Profundo:** Formulan el inpainting como un problema de generación condicional.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->128<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las arquitecturas comunes incluyen:
			- **Redes Encoder-Decoder:** A menudo con una estructura U-Net.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->135<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Redes Coarse-to-Fine:** Un enfoque de dos etapas donde una primera red genera un relleno burdo y una segunda red lo refina.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->126<!----><!----><!----><!----><!----><!----><!----><!----><!----> DeepFillv1 y v2 emplean esta estrategia.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->136<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Convoluciones Parciales/Gateadas:** Para abordar el problema de que las convoluciones estándar tratan los píxeles enmascarados como válidos, lo que genera artefactos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->88<!----><!----><!----><!----><!----><!----><!----><!----><!----> La **convolución parcial** <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->126<!----><!----><!----><!----><!----><!----><!----><!----><!----> enmascara la operación de convolución para considerar solo los píxeles válidos. La **convolución gateada (Gated Convolution)** <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->88<!----><!----><!----><!----><!----><!----><!----><!----><!---->, utilizada en DeepFillv2, generaliza esto aprendiendo una máscara suave (compuerta) para cada canal y ubicación espacial, permitiendo a la red seleccionar dinámicamente qué características utilizar.<!----><!----><!----><!----><!---->
			- **Atención Contextual:** Mecanismo que permite a la red "prestar" o copiar características de regiones conocidas distantes para ayudar a rellenar el hueco, capturando dependencias de largo alcance.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->137<!----><!----><!----><!----><!----><!----><!----><!----><!----> Utilizado en DeepFillv1/v2.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->136<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **LaMa (Large Mask Inpainting):** Diseñado específicamente para manejar máscaras grandes y arbitrarias de manera eficiente.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->124<!----><!----><!----><!----><!----><!----><!----><!----><!----> Sus componentes clave son <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->130<!----><!----><!----><!----><!----><!----><!----><!----><!---->:
				- **Convoluciones Rápidas de Fourier (Fast Fourier Convolutions, FFC) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->132<!----><!----><!----><!----><!----><!----><!----><!----><!---->:** Utilizan la Transformada Rápida de Fourier (FFT) para operar en el dominio de la frecuencia. Esto proporciona a la red un **campo receptivo que abarca toda la imagen** desde las primeras capas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->132<!----><!----><!----><!----><!----><!----><!----><!----><!----> Es crucial para capturar el contexto global necesario para rellenar agujeros grandes y para generar estructuras periódicas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->132<!----><!----><!----><!----><!----><!----><!----><!----><!----> También permite una sorprendente generalización a resoluciones más altas que las vistas durante el entrenamiento.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->132<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
				- **Pérdida Perceptual de Alto Campo Receptivo:** Una función de pérdida diseñada para evaluar la consistencia global.
				- **Máscaras de Entrenamiento Grandes:** El uso de máscaras grandes y diversas durante el entrenamiento mejora la robustez del modelo.
				  
				  <!----><!----><!----><!----><!---->
			- **DeepFillv2:** Un influyente modelo de inpainting <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->128<!----><!----><!----><!----><!----><!----><!----><!----><!----> que utiliza **convoluciones gateadas** <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->88<!----><!----><!----><!----><!----><!----><!----><!----><!----> y **atención contextual**.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->137<!----><!----><!----><!----><!----><!----><!----><!----><!----> Puede incorporar guía del usuario (p.ej., bocetos).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->88<!----><!----><!----><!----><!----><!----><!----><!----><!----> Utiliza un discriminador **SN-PatchGAN** <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->88<!----><!----><!----><!----><!----><!----><!----><!----><!---->, que aplica un discriminador con Normalización Espectral (SN) a parches densos de la imagen, enfocándose en la calidad local y la textura, y siendo estable para máscaras de forma libre.<!----><!----><!----><!----><!---->
			- **Modelos de Difusión:** Enfoques más recientes como RePaint <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->124<!----><!----><!----><!----><!----><!----><!----><!----><!----> adaptan modelos de difusión preentrenados (DDPMs) para inpainting. Funcionan denoising iterativamente la región enmascarada, condicionada por la región conocida. Pueden manejar bien máscaras arbitrarias.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->124<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
			  
			  Un desafío fundamental en inpainting, particularmente con máscaras grandes, es el **campo receptivo limitado** de las CNNs estándar.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->132<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las capas iniciales simplemente no pueden "ver" suficiente contexto de las regiones conocidas para tomar decisiones informadas sobre cómo rellenar un agujero grande. Técnicas como la atención contextual <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->137<!----><!----><!----><!----><!----><!----><!----><!----><!---->, que explícitamente toma prestadas características de ubicaciones distantes, y especialmente las FFC de LaMa <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->132<!----><!----><!----><!----><!----><!----><!----><!----><!---->, que logran un campo receptivo global desde el principio operando en el dominio de la frecuencia, son innovaciones arquitectónicas diseñadas específicamente para superar esta limitación.<!----><!----><!----><!----><!---->
			  
			  Otro aspecto clave es cómo manejar la **irregularidad de las máscaras** y los píxeles faltantes durante la convolución.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->88<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las convoluciones estándar, al aplicar el mismo filtro a píxeles válidos, inválidos y de borde, introducen artefactos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->88<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las convoluciones parciales <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->126<!----><!----><!----><!----><!----><!----><!----><!----><!----> y las convoluciones gateadas <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->88<!----><!----><!----><!----><!----><!----><!----><!----><!----> fueron desarrolladas para abordar esto, adaptando dinámicamente la operación de convolución en función de la validez de los píxeles (o de compuertas aprendidas), lo que resulta crucial para el éxito del inpainting de forma libre.<!----><!----><!----><!----><!---->
			  
			  **Aplicaciones:** Restauración de fotos (eliminar arañazos, objetos no deseados), edición de imágenes (eliminar marcas de agua), composición de escenas, creación de contenido visual.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->88<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- ### 3.3 Manipulación de Estilo: Transferencia de Estilo Neuronal (NST, AdaIN, WCT)
		  
		  La Transferencia de Estilo Neuronal (Neural Style Transfer, NST) es la tarea de recomponer una imagen (la imagen de contenido) con el estilo artístico (textura, colores, patrones de pincelada) de otra imagen (la imagen de estilo), preservando al mismo tiempo la estructura semántica de la imagen de contenido.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->143<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **NST Basada en Optimización (Gatys et al.):** El trabajo seminal <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->143<!----><!----><!----><!----><!----><!----><!----><!----><!----> utilizó una red CNN preentrenada (VGG). La **representación del contenido** se definió como las activaciones de los mapas de características en una o más capas profundas de la red. La **representación del estilo** se capturó mediante las correlaciones entre las activaciones de los mapas de características en varias capas, calculadas a través de las **matrices de Gram** (producto punto entre vectores de características aplanados).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->143<!----><!----><!----><!----><!----><!----><!----><!----><!----> Se sintetizaba una nueva imagen optimizándola iterativamente (p.ej., mediante descenso de gradiente) para minimizar una pérdida combinada: una pérdida de contenido (diferencia entre las características de contenido de la imagen sintetizada y la imagen de contenido) y una pérdida de estilo (diferencia entre las matrices de Gram de la imagen sintetizada y la imagen de estilo).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->143<!----><!----><!----><!----><!----><!----><!----><!----><!----> Este método es flexible (puede usar cualquier par de imágenes de contenido y estilo) pero muy lento computacionalmente.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->143<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **NST Basada en Redes Feed-Forward:** Para acelerar el proceso, se desarrollaron métodos que entrenan una red neuronal para realizar la transferencia de estilo en una única pasada hacia adelante.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->149<!----><!----><!----><!----><!----><!----><!----><!----><!----> Sin embargo, los primeros enfoques a menudo estaban limitados a transferir un único estilo fijo para el cual la red había sido entrenada.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->149<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Transferencia de Estilo Arbitraria/Universal:** El objetivo pasó a ser desarrollar métodos feed-forward que pudieran transferir el estilo de *cualquier* imagen de estilo dada en tiempo real, combinando la flexibilidad del método de optimización con la velocidad de las redes feed-forward.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->144<!----><!----><!----><!----><!----><!----><!----><!----><!----> Dos técnicas clave surgieron:
			- **AdaIN (Adaptive Instance Normalization) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->149<!----><!----><!----><!----><!----><!----><!----><!----><!---->:** Propuesta por Huang y Belongie.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->149<!----><!----><!----><!----><!----><!----><!----><!----><!----> La idea central es alinear las estadísticas de los mapas de características (media y varianza) del contenido con las de las características del estilo, canal por canal.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->149<!----><!----><!----><!----><!----><!----><!----><!----><!----> Dado un mapa de características de contenido x y un mapa de características de estilo y, AdaIN calcula:
			  AdaIN(x,y)=σ(y)(σ(x)x−μ(x)​)+μ(y)
			  donde μ(⋅) y σ(⋅) son la media y la desviación estándar calculadas espacialmente para cada canal.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->149<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esencialmente, normaliza las características de contenido (como en Instance Normalization, IN) y luego las re-escala y desplaza usando la media y desviación estándar del estilo. Se implementa dentro de una arquitectura encoder-decoder: el encoder (p.ej., VGG) extrae características, AdaIN las alinea en el espacio de características, y el decoder genera la imagen estilizada.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->149<!----><!----><!----><!----><!----><!----><!----><!----><!----> Es rápido y permite estilos arbitrarios.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->149<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **WCT (Whitening and Coloring Transforms) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->145<!----><!----><!----><!----><!----><!----><!----><!----><!---->:** Propuesto por Li et al..<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->144<!----><!----><!----><!----><!----><!----><!----><!----><!----> En lugar de solo igualar medias y varianzas (estadísticas marginales), WCT busca igualar la **matriz de covarianza** completa de los mapas de características, capturando así las correlaciones entre canales, que también son importantes para el estilo.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->144<!----><!----><!----><!----><!----><!----><!----><!----><!----> El proceso, aplicado a características extraídas por un encoder, consta de dos pasos <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->145<!----><!----><!----><!----><!----><!----><!----><!----><!---->:
				- **Whitening (Blanqueamiento):** Transforma las características de contenido centradas fˉ​c​ para que su matriz de covarianza sea la identidad, eliminando las correlaciones originales (estilo del contenido). Fórmula: f^​c​=Ec​Dc−1/2​EcT​fˉ​c​.
				- **Coloring (Coloreado):** Aplica la transformación inversa utilizando las estadísticas de las características de estilo centradas fˉ​s​ (específicamente, su matriz de covarianza Es​Ds​EsT​) a las características de contenido blanqueadas f^​c​. Fórmula: f^​cs​=Es​Ds1/2​EsT​f^​c​. Finalmente, se añade la media de las características de estilo ms​.
				  El resultado f^​cs​ se pasa luego al decoder. WCT puede aplicarse en múltiples niveles de características (multi-level WCT) para obtener mejores resultados.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->145<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
				  
				  <!----><!----><!----><!----><!---->
				  
				  <!----><!----><!----><!----><!---->
				  
				  Un concepto fundamental que subyace a estos métodos es que las **estadísticas de los mapas de características profundas** actúan como un proxy efectivo para el estilo perceptual.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->143<!----><!----><!----><!----><!----><!----><!----><!----><!----> Gatys utilizó estadísticas de segundo orden (matriz de Gram/covarianza). AdaIN utiliza momentos de primer y segundo orden (media, varianza). WCT iguala explícitamente la matriz de covarianza completa. Esto sugiere que el estilo, tal como lo perciben estas redes, reside en la distribución estadística y la correlación de las características a través de las ubicaciones espaciales, más que en los valores exactos de las características.<!----><!----><!----><!----><!---->
				  
				  La evolución de la NST también refleja un compromiso entre **velocidad, flexibilidad y calidad/control**. El método de Gatys era flexible pero lento.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->143<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las redes feed-forward iniciales eran rápidas pero limitadas en estilo.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->149<!----><!----><!----><!----><!----><!----><!----><!----><!----> AdaIN y WCT lograron un equilibrio al ofrecer velocidad y flexibilidad.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->144<!----><!----><!----><!----><!----><!----><!----><!----><!----> Sin embargo, la investigación continúa para mejorar la calidad (p.ej., fotorrealismo con PhotoWCT <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->146<!----><!----><!----><!----><!----><!----><!----><!----><!---->), el control más fino y el desentrelazamiento de aspectos específicos del estilo.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->147<!----><!----><!----><!----><!----><!----><!----><!----><!----> AdaIN, al igualar solo medias y varianzas, podría ser menos potente para capturar estilos complejos que WCT, que considera las covarianzas, aunque WCT podría tener más dificultades para preservar el contenido si no se equilibra adecuadamente.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->152<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- ### 3.4 Colorización Automática: Métodos (DeOldify) y Arquitecturas
		  
		  La colorización automática es la tarea de añadir color de forma realista a imágenes en escala de grises (blanco y negro).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->154<!----><!----><!----><!----><!----><!----><!----><!----><!----> Es un problema inherentemente mal planteado porque una única intensidad de gris puede corresponder a múltiples colores originales, lo que resulta en ambigüedad.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->155<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Arquitecturas:** Los modelos de colorización suelen emplear arquitecturas **Encoder-Decoder**.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->46<!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- El **Encoder** toma la imagen en escala de grises como entrada y la comprime en una representación de características de baja dimensión que captura información semántica y contextual.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->46<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- El **Decoder** toma estas características codificadas y reconstruye una imagen en color (p.ej., en espacio de color RGB o, más comúnmente para esta tarea, Lab, donde L es la luminancia - entrada - y ab son los canales de color - salida).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->46<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- Las **CNNs** son la base de estas redes, y las arquitecturas **U-Net** (encoder-decoder con conexiones skip entre capas correspondientes) son populares porque permiten que la información de bajo nivel (detalles finos) del encoder pase directamente al decoder, ayudando a generar resultados más nítidos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->136<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
		- **Funciones de Pérdida:** El objetivo es minimizar la diferencia entre la imagen coloreada predicha y la imagen en color original (verdad terreno) durante el entrenamiento.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->46<!----><!----><!----><!----><!----><!----><!----><!----><!----> Se utilizan pérdidas como el Error Cuadrático Medio (MSE) o la pérdida L1, a menudo calculadas en los canales de color (p.ej., ab en el espacio Lab).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->46<!----><!----><!----><!----><!----><!----><!----><!----><!----> Algunos enfoques tratan la colorización como un problema de clasificación, prediciendo una distribución de probabilidad sobre posibles colores para cada píxel.<!----><!----><!----><!----><!---->
		- **Modelos y Técnicas Clave:**
			- **Zhang et al. (2016, 2017):** Trabajos influyentes que utilizaron CNNs y a menudo formularon el problema como la predicción de distribuciones de color por píxel.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->154<!----><!----><!----><!----><!----><!----><!----><!----><!----> Sus modelos preentrenados (`eccv16`, `siggraph17`) todavía se utilizan.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->154<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **DeOldify:** Un enfoque de vanguardia específicamente diseñado para colorear fotos y vídeos antiguos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->154<!----><!----><!----><!----><!----><!----><!----><!----><!----> Utiliza una arquitectura **U-Net con un backbone ResNet** (ResNet34 para el modelo "Artistic", ResNet101 para "Stable" y "Video").<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->157<!----><!----><!----><!----><!----><!----><!----><!----><!----> Emplea una técnica de entrenamiento llamada **NoGAN**, que utiliza entrenamiento adversarial para mejorar la calidad perceptual y la viveza del color sin necesidad de un discriminador GAN típico, centrándose en la crítica de características.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->157<!----><!----><!----><!----><!----><!----><!----><!----><!----> Ofrece diferentes modelos preentrenados que equilibran la viveza del color ("Artistic") con la estabilidad y la prevención de colores poco realistas ("Stable") y la consistencia temporal para vídeo ("Video").<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->157<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **DDColor:** Otro modelo de colorización mencionado como alternativa o complemento a DeOldify.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->154<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Colorización de Vídeo Basada en Ejemplares:** Métodos como Deep Exemplar based Video Colorization (DeepEx), DeepRemaster y ColorMNet utilizan imágenes o fotogramas de referencia coloreados para guiar la colorización de un vídeo, lo que ayuda a mantener la consistencia temporal y a reducir el parpadeo (flickering).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->154<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  La **ambigüedad inherente** a la colorización es un desafío central.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->155<!----><!----><!----><!----><!----><!----><!----><!----><!----> Dado que la información de color se pierde en la conversión a escala de grises, el modelo debe inferir colores plausibles basándose en el contenido semántico (p.ej., "el cielo es azul", "la hierba es verde") y el contexto aprendido de un gran conjunto de datos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->46<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esto explica por qué los modelos pueden tener dificultades con objetos que tienen colores variables (como ropa o coches), a menudo produciendo colores desaturados o promediados.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->155<!----><!----><!----><!----><!----><!----><!----><!----><!----> La meta no es necesariamente recuperar el color original exacto (que es imposible sin información adicional), sino generar una colorización perceptualmente creíble.<!----><!----><!----><!----><!---->
			  
			  La existencia de diferentes modelos en DeOldify (Artistic, Stable, Video) subraya los **compromisos entre diferentes objetivos de calidad**.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->157<!----><!----><!----><!----><!----><!----><!----><!----><!----> El modelo "Artistic" busca colores vibrantes, posiblemente a expensas del realismo estricto. El modelo "Stable" prioriza evitar colores extraños o sobresaturados, lo que puede resultar en una apariencia más apagada pero más segura. El modelo "Video" se centra en la consistencia temporal para evitar el parpadeo, lo que podría comprometer ligeramente la calidad óptima de cada fotograma individual. Esto demuestra que no existe un único modelo de colorización "óptimo", sino que se necesitan modelos especializados según la aplicación (restauración artística vs. archivo histórico vs. película) y la estética deseada.<!----><!----><!----><!----><!---->
			  
			  **Aplicaciones:** Principalmente la restauración y mejora de fotografías y películas históricas en blanco y negro <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->154<!----><!----><!----><!----><!----><!----><!----><!----><!---->, pero también puede tener usos creativos.<!----><!----><!----><!----><!---->
		- ### 3.5 Comprensión de Escenas: Segmentación y Detección (Mask R-CNN, YOLOv7, SAM)
		  
		  La comprensión de escenas implica identificar y localizar objetos, así como entender la disposición espacial y semántica de una imagen. Las tareas clave incluyen:
		- **Detección de Objetos:** Localizar instancias de objetos mediante cuadros delimitadores (bounding boxes) y asignarles una etiqueta de clase.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->159<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Segmentación Semántica:** Asignar una etiqueta de clase a cada píxel de la imagen (p.ej., 'carretera', 'cielo', 'coche'), sin diferenciar entre instancias individuales de la misma clase.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->159<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Segmentación de Instancias:** Detectar objetos individuales y generar una máscara de segmentación a nivel de píxel para cada instancia detectada.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->159<!----><!----><!----><!----><!----><!----><!----><!----><!----> Es una combinación de detección y segmentación.<!----><!----><!----><!----><!---->
		- **Segmentación Panóptica:** Unifica la segmentación semántica y de instancias, asignando a cada píxel tanto una etiqueta de clase semántica como un ID de instancia (si pertenece a una instancia contable o "thing").<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->163<!----><!----><!----><!----><!----><!----><!----><!----><!----> Proporciona una comprensión completa de la escena, cubriendo tanto objetos ("things") como regiones amorfas ("stuff" como cielo, hierba).<!----><!----><!----><!----><!---->
		  
		  **Modelos Clave:**
		- **Mask R-CNN (Segmentación de Instancias) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->159<!----><!----><!----><!----><!----><!----><!----><!----><!---->:**
			- **Arquitectura:** Extiende la popular arquitectura de detección de objetos Faster R-CNN.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->159<!----><!----><!----><!----><!----><!----><!----><!----><!----> Es un enfoque de **dos etapas**:
				- Una Red de Propuestas de Región (Region Proposal Network, RPN) genera propuestas de regiones (RoIs) que probablemente contengan objetos.
				- Para cada RoI, la segunda etapa realiza **tres tareas en paralelo**: clasificación de la clase del objeto, regresión del cuadro delimitador y predicción de una **máscara de segmentación binaria**.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->159<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
				  
				  <!----><!----><!----><!----><!---->
			- **Rama de Máscara:** Una pequeña Red Totalmente Convolucional (Fully Convolutional Network, FCN) que toma las características de la RoI y predice la máscara píxel a píxel.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->159<!----><!----><!----><!----><!----><!----><!----><!----><!----> El uso de una FCN preserva la información espacial.<!----><!----><!----><!----><!---->
			- **RoIAlign:** Una mejora crucial sobre el RoIPooling de Faster R-CNN.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->159<!----><!----><!----><!----><!----><!----><!----><!----><!----> RoIPooling introduce desalineaciones debido a la cuantificación espacial al extraer características de las RoIs. RoIAlign evita esta cuantificación utilizando interpolación bilineal para calcular con precisión los valores de las características en ubicaciones de muestreo de punto flotante dentro de la RoI.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->159<!----><!----><!----><!----><!----><!----><!----><!----><!----> Esta alineación precisa es fundamental para la segmentación a nivel de píxel.<!----><!----><!----><!----><!---->
			- **Predicción Desacoplada:** La rama de máscara predice K máscaras (una para cada clase), y la rama de clasificación predice la etiqueta de clase. La máscara correspondiente a la clase predicha se selecciona como salida final.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->159<!----><!----><!----><!----><!----><!----><!----><!----><!----> Utiliza sigmoides por píxel y pérdida de entropía cruzada binaria.<!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
		- **Familia YOLO (Detección de Objetos):**
			- **Arquitectura:** Detectores de **una etapa** conocidos por su alta velocidad, adecuados para aplicaciones en tiempo real.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->17<!----><!----><!----><!----><!----><!----><!----><!----><!----> YOLO divide la imagen en una cuadrícula y predice cuadros delimitadores, confianzas de objeto y probabilidades de clase directamente para cada celda de la cuadrícula.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->161<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Evolución:** Ha habido numerosas versiones (YOLOv1 a YOLOv7 y más allá), cada una introduciendo mejoras en la arquitectura (p.ej., Darknet, CSPNet), mecanismos de predicción (p.ej., anchor boxes, predicción multi-escala) y estrategias de entrenamiento ("Bag of Freebies", "Bag of Specials").<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->161<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **YOLOv7:** Un modelo que alcanzó el estado del arte en velocidad y precisión para detección en tiempo real en su momento.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->17<!----><!----><!----><!----><!----><!----><!----><!----><!----> Sus innovaciones incluyen <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->168<!----><!----><!----><!----><!----><!----><!----><!----><!---->:
				- **E-ELAN (Extended Efficient Layer Aggregation Network):** Una arquitectura de red eficiente que mejora el aprendizaje en redes profundas controlando las rutas de gradiente y utilizando convolución de grupo para aumentar la cardinalidad de las características.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->17<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
				- **Escalado Compuesto para Modelos Basados en Concatenación:** Métodos de escalado de modelo (ajuste de profundidad/ancho) que tienen en cuenta cómo afecta el escalado a las capas en arquitecturas basadas en concatenación (como DenseNet o E-ELAN).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->17<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
				- **"Bag-of-Freebies" Entrenables:** Técnicas que mejoran la precisión sin aumentar el costo de inferencia, aprendidas durante el entrenamiento. Incluyen:
					- **Re-parametrización Planificada:** Uso estratégico de convoluciones re-parametrizadas (como RepConv, pero sin la conexión identidad, RepConvN) para fusionar múltiples ramas en una sola capa en tiempo de inferencia.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->18<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
					- **Asignación de Etiquetas Guiada Coarse-to-Fine:** Uso de cabezas auxiliares durante el entrenamiento con etiquetas "gruesas" (más permisivas) para ayudar a la cabeza principal ("lead head") a aprender con etiquetas "finas" (más precisas).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->17<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
					  
					  <!----><!----><!----><!----><!---->
		- **Segment Anything Model (SAM) (Segmentación Promptable) <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->171<!----><!----><!----><!----><!----><!----><!----><!----><!---->:**
			- **Concepto:** Un **modelo fundacional** para segmentación, preentrenado en un conjunto de datos masivo (SA-1B, con más de mil millones de máscaras).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->171<!----><!----><!----><!----><!----><!----><!----><!----><!----> Diseñado para generalización **zero-shot** a diversas tareas y dominios de segmentación mediante **prompts**.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->174<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Arquitectura:** Consta de tres componentes principales <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->171<!----><!----><!----><!----><!----><!----><!----><!----><!---->:
				- **Codificador de Imagen:** Una red pesada (basada en Vision Transformer, ViT, a menudo preentrenada con MAE) que procesa la imagen una sola vez para generar un embedding de imagen de alta calidad.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->173<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
				- **Codificador de Prompt:** Una red ligera que codifica eficientemente varios tipos de prompts de entrada (puntos, cuadros delimitadores, máscaras aproximadas, texto) en embeddings.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->173<!----><!----><!----><!----><!----><!----><!----><!----><!----> Los puntos y cajas usan codificaciones posicionales; el texto puede usar CLIP; las máscaras se procesan con convoluciones.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->178<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
				- **Decodificador de Máscara:** Una red ligera (basada en Transformer) que combina el embedding de la imagen y los embeddings del prompt para predecir máscaras de segmentación de alta calidad de forma muy rápida (aprox. 50 ms), permitiendo la interactividad en tiempo real.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->173<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
				  
				  <!----><!----><!----><!----><!---->
			- **Manejo de Ambigüedad:** Para un prompt ambiguo (p.ej., un clic en una camiseta que podría referirse a la camiseta o a la persona), SAM está diseñado para predecir múltiples máscaras válidas y plausibles.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->174<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Motor de Datos (Data Engine):** SAM se utilizó de forma interactiva y automática para generar su propio conjunto de datos de entrenamiento masivo SA-1B, en un ciclo de anotación asistida por modelo.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->174<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- **Aplicaciones y Limitaciones:** Útil para segmentación general, transferencia zero-shot, asistencia en anotación de datos <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->174<!----><!----><!----><!----><!----><!----><!----><!----><!---->, análisis de imágenes médicas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->172<!----><!----><!----><!----><!----><!----><!----><!----><!----> Puede tener dificultades con detalles muy finos o políticas de anotación específicas del dominio.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->173<!----><!----><!----><!----><!----><!----><!----><!----><!----> Vulnerable a ataques adversariales.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->181<!----><!----><!----><!----><!----><!----><!----><!----><!----> Se requiere ajuste fino (fine-tuning) para tareas específicas con etiquetas de clase (p.ej., SamLoRA <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->177<!----><!----><!----><!----><!----><!----><!----><!----><!---->). SAM 2 busca mejorar la granularidad y el contexto.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->172<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
			  
			  La aparición de SAM marca una transición significativa hacia **modelos fundacionales** en visión por computador, análoga a la de los LLMs en NLP.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->172<!----><!----><!----><!----><!----><!----><!----><!----><!----> En lugar de entrenar modelos específicos para cada tarea desde cero, el paradigma se desplaza hacia el preentrenamiento de modelos masivos y altamente generales en datos a gran escala, que luego se adaptan a tareas específicas mediante prompting o ajuste fino mínimo. Esto cambia fundamentalmente el flujo de trabajo de desarrollo en CV.<!----><!----><!----><!----><!---->
			  
			  Por otro lado, el éxito de YOLOv7 ilustra que el rendimiento de vanguardia a menudo resulta de una combinación optimizada tanto de **innovaciones arquitectónicas** (como E-ELAN y la re-parametrización planificada) como de **estrategias de entrenamiento sofisticadas** (como el "bag-of-freebies" entrenable, incluyendo la asignación de etiquetas guiada coarse-to-fine).<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->17<!----><!----><!----><!----><!----><!----><!----><!----><!----> No basta con una buena arquitectura; el proceso de entrenamiento debe diseñarse cuidadosamente para extraer el máximo rendimiento, especialmente en detectores de una etapa que manejan la compleja tarea de la asignación de etiquetas y el equilibrio de pérdidas entre diferentes escalas y tipos de objetos.<!----><!----><!----><!----><!---->
		- ## Sección 4: Técnicas de Generación Condicional de Imágenes
		  
		  Mientras que los modelos generativos no condicionales aprenden la distribución general de los datos, la generación condicional permite dirigir el proceso de síntesis basándose en información adicional, como texto, bocetos, mapas de segmentación u otras imágenes.
		- ### 4.1 Síntesis de Texto a Imagen: Modelos Guiados por CLIP (DALL-E, Imagen, Midjourney)
		  
		  La generación de imágenes a partir de descripciones textuales (Text-to-Image, T2I) es una de las aplicaciones más impactantes y populares de la IA generativa.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->183<!----><!----><!----><!----><!----><!----><!----><!----><!----> Permite a los usuarios crear imágenes complejas y novedosas simplemente describiéndolas en lenguaje natural.<!----><!----><!----><!----><!---->
		  
		  **Componentes Centrales:** Los modelos T2I típicamente constan de <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->187<!----><!----><!----><!----><!----><!----><!----><!----><!---->:<!----><!----><!----><!----><!---->
		- **Codificador de Texto (Text Encoder):** Procesa el prompt de texto de entrada y lo convierte en una representación numérica (embedding) que captura su significado semántico.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->183<!----><!----><!----><!----><!----><!----><!----><!----><!----> Los modelos Transformer son comunes para esta tarea.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->185<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Generador de Imágenes (Image Generator):** Toma el embedding de texto (y a menudo un vector de ruido latente) como entrada y genera una imagen condicionada por la semántica del texto.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->183<!----><!----><!----><!----><!----><!----><!----><!----><!----> Las arquitecturas generativas subyacentes han evolucionado desde GANs y VAEs hacia Transformers y, predominantemente en la actualidad, Modelos de Difusión.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->185<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **El Papel Crucial de CLIP:**
		  
		  El modelo **CLIP (Contrastive Language-Image Pre-training)** <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->183<!----><!----><!----><!----><!----><!----><!----><!----><!----> ha sido fundamental para el avance de T2I. CLIP aprende un **espacio de embedding multimodal** donde las representaciones de imágenes y sus descripciones textuales correspondientes están alineadas; es decir, el embedding de una imagen de un perro está cerca del embedding del texto "una foto de un perro".<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->183<!----><!----><!----><!----><!----><!----><!----><!----><!----> Se entrena de forma contrastiva en pares masivos de imagen-texto (p.ej., 400 millones en el entrenamiento original de OpenAI <!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->189<!----><!----><!----><!----><!----><!----><!----><!----><!---->) para maximizar la similitud (p.ej., coseno) entre pares correctos y minimizarla entre pares incorrectos.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->189<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  En los modelos T2I, el **codificador de texto de CLIP** se utiliza muy a menudo para generar los embeddings de texto que guían la síntesis de imágenes.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->183<!----><!----><!----><!----><!----><!----><!----><!----><!----> Modelos de difusión como Stable Diffusion utilizan estos embeddings CLIP para condicionar el proceso de denoising a través de mecanismos de atención cruzada.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->96<!----><!----><!----><!----><!----><!----><!----><!----><!----> DALL-E 2 también depende en gran medida de los embeddings CLIP tanto en su red *prior* como en su decodificador de difusión.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->190<!----><!----><!----><!----><!----><!----><!----><!----><!----> Además, la propia puntuación de similitud de CLIP se puede usar para guiar activamente el proceso de generación (a veces llamado "CLIP guidance"), empujando la imagen generada a ser más similar al prompt según la métrica de CLIP.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->183<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		  
		  **Modelos T2I Emblemáticos:**
		- **DALL-E / DALL-E 2 / DALL-E 3 (OpenAI):**
			- *DALL-E 1:* Basado en una arquitectura Transformer similar a GPT-3, operaba directamente sobre tokens de texto e imagen.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->196<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *DALL-E 2:* Utiliza un enfoque basado en difusión y CLIP.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->185<!----><!----><!----><!----><!----><!----><!----><!----><!----> Consta de dos etapas principales: 1) Una red **prior** (un modelo de difusión o autorregresivo) que mapea el embedding de texto CLIP a un embedding de imagen CLIP predicho. 2) Un **decodificador** (un modelo de difusión basado en GLIDE, modificado para ser condicionado por el embedding de imagen CLIP) que genera la imagen final a partir del embedding de imagen predicho.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->190<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *DALL-E 3:* Versión más reciente con mejoras en calidad y seguimiento de prompts.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->197<!----><!----><!----><!----><!----><!----><!----><!----><!----> OpenAI también ofrece ahora `gpt-image-1` como su modelo más avanzado.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->197<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *Características:* Conocido por su capacidad para generar imágenes creativas, combinar conceptos no relacionados y seguir descripciones detalladas, aunque puede tener dificultades con relaciones espaciales complejas o conteo preciso.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->196<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Imagen (Google):**
			- *Arquitectura:* Utiliza una **cascada de modelos de difusión** que operan en el espacio de píxeles.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->99<!----><!----><!----><!----><!----><!----><!----><!----><!----> Un modelo base genera una imagen de baja resolución (p.ej., 64x64) condicionada por el texto, y modelos de difusión de súper-resolución posteriores la escalan a resoluciones más altas (p.ej., 256x256, 1024x1024), también condicionados por el texto.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->99<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *Codificador de Texto:* Una de sus principales contribuciones fue demostrar la eficacia de usar **modelos de lenguaje grandes (LLMs) preentrenados solo con texto y congelados** (como T5-XXL) como codificadores de texto.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->99<!----><!----><!----><!----><!----><!----><!----><!----><!----> Descubrieron que escalar el tamaño del LLM mejoraba la fidelidad de la muestra y la alineación texto-imagen más que escalar el tamaño de las U-Nets de difusión.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->99<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			- *Calidad:* Logró un fotorrealismo y una comprensión del lenguaje excepcionales, superando a modelos anteriores en benchmarks y evaluaciones humanas.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->99<!----><!----><!----><!----><!----><!----><!----><!----><!----> Imagen 3 es la versión más reciente.<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->199<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
		- **Stable Diffusion (Stability AI):**
			- *Arquitectura:* Modelo de **Difusión Latente (LDM)** de código abierto [<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->1<!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!----><!---->
			  
			  <!----><!----><!----><!----><!---->
			  
			  <!--EndFragment-->
-
- *   **Técnicas de Generación de Imagen Más Detalladas:** Profundizar en algunas de las técnicas de generación de imágenes.
	- *   **GANs (Redes Generativas Antagónicas):** Mencionar diferentes tipos de GANs como StyleGAN (para generar rostros realistas con un control granular sobre el estilo), CycleGAN (para traducción imagen-a-imagen sin necesidad de pares de datos alineados, como convertir fotos de caballos en cebras), y Deepfakes (y las implicaciones éticas de su uso).
	      *   **VAEs (Autoencoders Variacionales):** Enfatizar su uso para la generación2 y Stable Diffusion y su capacidad para generar imágenes fotorrealistas a partir de descripciones textuales. Su proceso iterativo de "difusión" y "desdifusión" permite un control más fino sobre el proceso generativo.
	  *   **Realidad Aumentada (AR) y Realidad Virtual (VR):** Integrar la IA en el contexto de AR/VR:
	      *   **Seguimiento basado en IA:** Utilizar IA para un seguimiento más preciso de objetos y personas en AR/VR, incluso en entornos complejos.
	      *   **Generación de contenido AR/VR:**  Emplear IA para generar contenido 3D virtual de forma automática, basándose en datos del mundo real o en instrucciones del usuario.
	      *   **Avatares impulsados por IA:**  Crear avatares realistas y animados que responden a las expresiones faciales y al lenguaje corporal del usuario.
	  *   **Aplicaciones E de conducción autónoma, detección de objetos en carretera, reconocimiento de señales de tráfico, asistencia al conductor (ADAS).
	      *   **Agricultura:** Detección de enfermedades en cultivos, monitorización del crecimiento de las plantas, optimización del riego y la fertilización.
	      *   **Seguridad:** Vigilancia por video inteligente, reconocimiento facial para control de acceso, detección de actividades sospechosas.
	      *   **Minoristas:** Reconocimiento facial de clientes, análisis de flujo de personas en tiendas, optimización del diseño de tiendas.
	      *   **Arte y Diseño:** Asistencia a artistas y diseñadores en la creación de obras de arte, generación de nuevos estilos artísticos.
	  *   **Preprocesamiento y Aumento de Datos (Data Augmentation):**
	      *   Destacar la importancia del preprocesamiento de las imágenes (normalización, ajuste de contraste, etc.) para mejorar el rendimiento de los modelos de IA.
	      *   Mencionar las técnicas de aumento de datos (rotación, escalado, recorte, flip, etc.) para aumentar la cantidad de datos de entrenamiento y mejorar la robustez de los modelos.
	- *   **Consideraciones Éticas y Sesgos:**
		- *   **Sesgos en los datos de entrenamiento:** Reconocer que los modelos de IA pueden heredar los sesgos presentes en los datos con los que se entrenan.  Por ejemplo, un modelo de reconocimiento facial entrenado principalmente con imágenes de personas de una raza específica puede tener un rendimiento deficiente en personas de otras razas.
		      *   **Privacidad:** Abordar las implicaciones de privacidad del uso de tecnologías de reconocimiento facial y vigilancia por video.
		      *   **Desinformación:**  Considerar los riesgos del uso de GANs y otras técnicas de generación de imágenes para crear deepfakes y propagar desinformación.
		      *   **Responsabilidad algorithms**:  Quien es responsable cuando un algoritmo de IA basado en análisis de imágenes toma una decisión incorrecta, especialmente en aplicaciones críticas como la medicina o la conducción autónoma?
	- **IA e Imagen: Un Panorama Amplio**
	  
	  Este documento explora las diversas tecnologías que convergen en la intersección de la Inteligencia Artificial (IA) y el procesamiento de imágenes, destacando tanto las técnicas fundamentales como las aplicaciones emergentes.
	  
	  **1.  Fundamentos del Procesamiento de Imágenes en IA**
	  
	  *   **Visión Artificial (Computer Vision):** El campo que permite a las máquinas "ver" e interpretar imágenes del mundo real.
	  *   **Aprendizaje Profundo (Deep Learning):**  La base de muchas de las técnicas más avanzadas en visión artificial, utilizando redes neuronales profundas para aprender patrones complejos en las imágenes.
	  *   **Conjuntos de Datos (Datasets):** Colecciones extensas de imágenes etiquetadas, cruciales para entrenar modelos de IA.  Ejemplos: ImageNet, COCO, MNIST.
	  *   **Herramientas y Frameworks:**  Bibliotecas de software que facilitan el desarrollo de aplicaciones de visión artificial. Ejemplos: TensorFlow, PyTorch, OpenCV, Keras.
	- **2. Tareas Clave en el Procesamiento de Imágenes con IA**
		- *   **Clasificación de Imágenes:** Asignar una etiqueta a una imagen completa (ej. "gato", "perro", "coche").
		  *   **Detección de Objetos:** Identificar y localizar múltiples objetos dentro de una imagen, dibujando cuadros delimitadores alrededor de cada uno (ej. detectar personas, coches y señales de tráfico en una calle).
		  *   **Segmentación Semántica:** Clasificar cada píxel de una imagen, asignándole una etiqueta a cada uno (ej. separar el cielo, la carretera, los edificios y los árboles en una imagen).
		  *   **Segmentación de Instancias:**  Similar a la segmentación semántica, pero diferenciando cada instancia de un objeto (ej. identificar y separar cada coche individualmente en una imagen).
		  *   **Reconocimiento Facial:** Identificar y verificar la identidad de una persona a partir de su rostro.
		  *   **Restauración de Imágenes:** Mejorar la calidad de imágenes dañadas o degradadas (ej. eliminar ruido, reconstruir partes faltantes).
		  *   **Descripción de Imágenes (Image Captioning):** Generar una descripción textual de una imagen.
		- **
	- 3. Técnicas Avanzadas de Generación de Imágenes**
		- *   **Redes Generativas Antagónicas (GANs):**
		      *   **Concepto:**  Dos redes neuronales, un generador y un discriminador, compiten entre sí. El generador crea imágenes falsas, y el discriminador intenta distinguir entre las imágenes reales y las falsas. Este proceso iterativo mejora la calidad de las imágenes generadas.
		      *   **Tipos:**
		          *   **StyleGAN:** Genera rostros increíblemente realistas con control preciso sobre atributos como edad, género, peinado, etc.
		          *   **CycleGAN:** Permite la traducción imagen-a-imagen sin datos de entrenamiento pareados (ej. convertir fotos de caballos en cebras, imágenes de día en imágenes de noche).
		          *   **Deepfakes:**  Aprovecha GANs para superponer rostros de una persona sobre el cuerpo de otra en un video, con aplicaciones preocupantes en la generación de desinformación.
		      *   **Ejemplos:** Generar rostros humanos, crear arte digital, modificar el estilo de una foto.
		- *   **Autoencoders Variacionales (VAEs):**
		      *   **Concepto:**  Redes que aprenden a codificar una imagen en un espacio latente de baja dimensión y luego decodificarla. Esto permite la generación de nuevas imágenes variando los puntos en el espacio latente.  Se utilizan para la generación de imágenes, la reducción de dimensionalidad y la detección de anomalías.
		      *   **Ventajas:**  Mayor estabilidad que las GANs, pero a menudo generan imágenes menos nítidas.
		  *
		- **Modelos de Difusión (Diffusion Models):**
		      *   **Concepto:**  Añaden ruido gaussiano progresivamente a una imagen hasta convertirla en ruido puro. Luego, una red neuronal aprende a "desdifuminar" la imagen, eliminando gradualmente el ruido para reconstruir la imagen original.
		      *   **Ejemplos:** Stable Diffusion, DALL-E 2, Midjourney. Capacidad para generar imágenes fotorrealistas a partir de descripciones textuales (text-
	- ## Herramientas de inteligencia artificial generativa de imagen
		- **Generación de Imagen a partir de Texto (Text-to-Image):**
			- Estas son quizás las herramientas más conocidas. Permiten crear imágenes sorprendentes simplemente escribiendo una descripción textual (prompt).
			- * **DALL-E 2 (OpenAI):** Una de las pioneras y más potentes. Conocida por su creatividad y capacidad para generar imágenes diversas y con estilos variados. Requiere una cuenta y ofrece créditos gratuitos al inicio.
			- * **Midjourney:** Especialmente popular en comunidades creativas por su estética a menudo artística y onírica. Funciona a través de un servidor de Discord y requiere suscripción para un uso extendido.
			- * **Stable Diffusion (Stability AI):** Un modelo de código abierto que ha impulsado una gran cantidad de herramientas y aplicaciones. Es altamente personalizable y se puede ejecutar localmente o a través de servicios en la nube.
			- * **Dream by WOMBO:** Una aplicación móvil fácil de usar que permite generar imágenes a partir de texto con diferentes estilos artísticos. Muy accesible para principiantes.
			  * **NightCafe Creator:** Una plataforma web que ofrece varios modelos de IA, incluyendo Stable Diffusion y DALL-E 2, con opciones de personalización y comunidad.
			  * **Lex modelos de IA, incluyendo Stable Diffusion, con opciones para editar y refinar imágenes.
		- **Edición y Manipulación de Imágenes con IA:**
			- Estas herramientas utilizan la IA para modificar, mejorar o completar imágenes existentes.
			- * **Generative Fill (Adobe Photoshop):** Una característica integrada en Photoshop que permite eliminar objetos, añadir elementos o completar áreas de forma generativa basándose en el contenido circundante o una descripción.
			- Clipdrop (Stability AI):** Ofrece una variedad de herramientas con IA, incluyendo eliminación de fondo, superresolución, iluminación y limpieza de fotos.
			- RunwayML:** Una plataforma potente para edición de video e imagen con IA, incluyendo herramientas de generación de video a partir de texto y edición de video asistida por IA.
		- **Otras Herramientas y Enfoques:**
		  
		  * **ControlNet:** Una extensión para modelos de difusión que permite ejercer un mayor control sobre la estructura, pose o composición de las imágenes generadas.
		  * **Dreambooth:** Una técnica para entrenar modelos de difusión en conjuntos de datos pequeños (por ejemplo, tus propias fotos) para generar imágenes de un sujeto o estilo específico.
		  * **LoRA (Low-Rank Adaptation):** Un método más eficiente para entrenar modelos en datos específicos, permitiendo crear estilos o sujetos personalizados sin necesidad de un entrenamiento completo.
		- ## Aspectos a Considerar al Elegir una Herramienta
			- * **Facilidad de Uso:** ¿Qué conceptos?
			- * **Costo:** ¿Es gratuita, de pago por uso o con suscripción?
			- * **Comunidad y Recursos:** ¿Hay una comunidad activa donde puedas encontrar ayuda y tutoriales?
			- * **Control:** ¿Qué tan granular es el control que tienes sobre el proceso de generación?
			- **Dónde Encontrar Más Información y Probar:**
			- * **Sitios web oficiales de las herramientas:** Explora las galerías de ejemplos y lee sobre las características.
			- **Plataformas de comunidad:** Reddit (r/midjourney, r/stablediffusionai), Discord (servidores oficiales de Midjourney, Stable Diffusion, etc.).
			- * **Sitios web de noticias tecnológicas y blogs especializados:** Suelen revisar y comparar nuevas herramientas.
			- * **Tutoriales en YouTube:** Hay muchísimos videos explicando cómo usar estas herramientas.
			  
			  La IA generativa de imagen está en constante evolución, con nuevas herramientas y técnicas surgiendo rápidamente. Te animo a explorar estas opciones y experimentar para encontrar las que mejor se adapten a tus necesidades y creatividad. ¡Las posibilidades son infinitas!
- ## Tecnologías relacionadas con inteligencia artificial y uso procesamiento modificación y generación de imagen
- Las principales tecnologías relacionadas con Inteligencia Artificial (IA) e Imagen se pueden clasificar en las siguientes categorías:
- **1. Visión Artificial (Computer Vision):**
	- *   **Aprendizaje Profundo (Deep Learning):** Es la base de la mayoría de las técnicas modernas de visión artificial. Se utilizan Redes Neuronales Convolucionales (CNNs) para extraer características relevantes de las imágenes y realizar tareas como:
	      *   **Clasificación de imágenes:** Asignar una etiqueta a una imagen completa (por ejemplo, "gato", "perro", "coche").  Arquitecturas populares: ResNet, VGGNet, Inception.
	      *   **Detección de objetos:** Identificar y localizar múltiples objetos dentro de una imagen (por ejemplo, detectar todos los coches, peatones y señales de tráfico en una calle).  Arquitecturas populares: YOLO, SSD, Faster R-CNN.
	      *   **Segmentación semántica:** Asignar una etiqueta a cada píxel de una imagen, dividiéndola en regiones significativas (por ejemplo, separar el cielo, la carretera, los árboles y los edificios).  Arquitecturas populares: U-Net, DeepLab.
	      *   **Segmentación de instancias:**  Similar a la segmentación semántica, pero además distingue entre diferentes instancias del mismo objeto (por ejemplo, separar cada coche individual en una imagen).  Arquitecturas populares: Mask R-CNN.
	      *   **Reconocimiento facial:** Identificar personas específicas en imágenes o videos.  Se basa en el análisis de características faciales y la comparación con bases de datos.
	      *   **Estimación de pose:**  Determinar la posición y orientación de objetos o cuerpos humanos en imágenes.
	      *   **Generación de imágenes:**  Crear imágenes sintéticas a partir de texto, ruido aleatorio u otras imágenes.  Arquitecturas populares: GANs (Redes Generativas Antagónicas), VAEs (Autoencoders Variacionales), Diffusion Models.
	- **Visión 3D:**
		- *   **Reconstrucción 3D:**  Crear modelos tridimensionales a partir de imágenes o videos.  Se utilizan técnicas como la fotogrametría, la visión estereoscópica y la SLAM (Simultaneous Localization and Mapping).
		      *   **Captura de movimiento (Motion Capture):**  Registrar el movimiento de objetos o personas en el espacio 3D.  Puede basarse en visión artificial o en sensores especializados.
- ## **2. Aprendizaje Automático (Machine Learning):**
	- *   **Aprendizaje Supervisado:**  Se entrena un modelo con datos etiquetados (por ejemplo, imágenes etiquetadas con la clase a la que pertenecen).  Se utiliza para clasificación, detección de objetos, segmentación, etc.  El aprendizaje profundo es un tipo de aprendizaje supervisado.
	  *   **Aprendizaje No Supervisado:**  Se entrena un modelo con datos no etiquetados para descubrir patrones ocultos.  Se utiliza para agrupamiento (clustering) de imágenes, reducción de dimensionalidad y detección de anomalías.  Ejemplos: k-means, PCA, t-SNE.
	  *   **Aprendizaje por Refuerzo:**  Un agente aprende a tomar decisiones en un entorno para maximizar una recompensa.  Puede utilizarse para tareas como la navegación autónoma de robots.
	  *   **Transfer Learning:**  Utilizar un modelo pre-entrenado en un conjunto de datos grande (por ejemplo, ImageNet) y ajustarlo para una tarea específica con un conjunto de datos más pequeño.
-